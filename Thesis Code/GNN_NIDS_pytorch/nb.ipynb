{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GNN' from 'c:\\\\TFG\\\\GNN-NIDS-main\\\\GNN_NIDS_pytorch\\\\GNN.py'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import GNN\n",
    "from torch.utils.data import Dataset\n",
    "import IDS2017_Dataset\n",
    "importlib.reload(GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import make_or_restore_model\n",
    "import generator\n",
    "importlib.reload(generator)\n",
    "from generator import generate_dataset\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = configparser.ConfigParser()\n",
    "params._interpolation = configparser.ExtendedInterpolation()\n",
    "params.read('./config.ini')\n",
    "\n",
    "forceCPU = False\n",
    "\n",
    "useCuda = torch.cuda.is_available() and not forceCPU\n",
    "if(useCuda):\n",
    "    device = torch.cuda.current_device()\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = None\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "importlib.reload(preprocess)\n",
    "from preprocess import sortByTimestamp, checkSorted, shortenDataset\n",
    "# Aquí es transformen i es posen en format convenient el timestamp del flow, s'ordenen les files segons el temps en ordre ascendent i es treuen files afegides amb capçaleres que hi havia de per mig\n",
    "\n",
    "# df = sortByTimestamp(os.path.abspath(params[\"DIRECTORIES\"][\"train\"]), os.path.abspath(params[\"DIRECTORIES\"][\"train_sorted\"]) + '/train_dataset.csv')\n",
    "\n",
    "# print(checkSorted(df))\n",
    "\n",
    "# df.to_csv(os.path.abspath(params[\"DIRECTORIES\"][\"train_sorted\"]) + '/train_dataset.csv', header=False, index=False)\n",
    "\n",
    "# df = sortByTimestamp(os.path.abspath(params[\"DIRECTORIES\"][\"validation\"]), os.path.abspath(params[\"DIRECTORIES\"][\"train_sorted\"]) + '/train_dataset.csv')\n",
    "\n",
    "# print(checkSorted(df))\n",
    "\n",
    "# df.to_csv(os.path.abspath(params[\"DIRECTORIES\"][\"validation_sorted\"]) + '/eval_dataset.csv', header=False, index=False)\n",
    "\n",
    "# shortenDataset(os.path.abspath(params[\"DIRECTORIES\"][\"train\"]), os.path.abspath(params[\"DIRECTORIES\"][\"train_short\"]) + '/train_dataset.csv')\n",
    "# shortenDataset(os.path.abspath(params[\"DIRECTORIES\"][\"validation\"]), os.path.abspath(params[\"DIRECTORIES\"][\"validation_short\"]) '/eval_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(IDS2017_Dataset)\n",
    "train_ds = IDS2017_Dataset.IDS2017_Dataset(os.path.abspath(params[\"DIRECTORIES\"][\"train\"]))\n",
    "eval_ds = IDS2017_Dataset.IDS2017_Dataset(os.path.abspath(params[\"DIRECTORIES\"][\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2000\n",
    "epochs = 2000\n",
    "validation_steps = 600\n",
    "batch_size = 16\n",
    "steps_per_epoch = 1600\n",
    "window = 200\n",
    "lr = float(params['HYPERPARAMETERS']['learning_rate'])\n",
    "decay_rate = float(params['HYPERPARAMETERS']['decay_rate'])\n",
    "decay_steps = int(params['HYPERPARAMETERS']['decay_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_params = {   \n",
    "                    'batch_size': batch_size,\n",
    "                    'num_workers': 0\n",
    "                }\n",
    "training_generator = DataLoader(train_ds, **loader_params)\n",
    "evaluating_generator = DataLoader(eval_ds, **loader_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GNN.GNN(params)\n",
    "if(useCuda):\n",
    "    model.to(device=device)\n",
    "    print(next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import glob\n",
    "path = os.path.abspath(params['DIRECTORIES']['logs'])+ \"\\\\ckpt\"\n",
    "files = glob.glob(path + '/*.pt')\n",
    "if(len(files) > 0):\n",
    "    files = [(path, path.split('\\\\')[-1]) for path in files]\n",
    "    files = [(path, '.'.join(el.split('.')[1:-1])) for (path, el) in files]\n",
    "    files = [(path, el.split('-')) for (path, el) in files]\n",
    "    files = [(path, {'Epoch': int(el[0]), 'Loss': float(el[1])}) for (path, el) in files]\n",
    "    files.sort(key=lambda x: (x[1]['Epoch'], x[1]['Loss']), reverse=True)\n",
    "    filepath = files[0][0]\n",
    "\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer = Adam(params=model.parameters() , lr=float(params['HYPERPARAMETERS']['learning_rate']))\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    loadedEpoch = checkpoint['epoch']\n",
    "    startingLoss = checkpoint['loss']\n",
    "    # scheduler = ExponentialLR(optimizer=optimizer, gamma=float(params['HYPERPARAMETERS']['decay_rate']))\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    startingEpoch = loadedEpoch + 1\n",
    "else:\n",
    "    startingEpoch = 1\n",
    "    optimizer = Adam(params=model.parameters() , lr=lr)\n",
    "    # scheduler = ExponentialLR(optimizer=optimizer, gamma=float(params['HYPERPARAMETERS']['decay_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_epoch(gen,epoch_index):\n",
    "    model.train()\n",
    "    running_accuracy = 0.0 \n",
    "    running_loss = 0.\n",
    "    running_weighted_f1 = 0.0\n",
    "    running_macro_f1 = 0.0\n",
    "    total = 0\n",
    "    loss = CrossEntropyLoss()\n",
    "    for i, batch in enumerate(gen):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        (input, labels) = batch\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        window_size = labels.size(1)\n",
    "        number_classes = labels.size(2)\n",
    "        output = model(input)\n",
    "\n",
    "        # The label with the highest value will be our prediction \n",
    "        _, predicted = torch.max(output, 2)\n",
    "        _, labelsIndex = torch.max(labels, 2)\n",
    "        shape = [batch_size, window_size, 1]\n",
    "        \n",
    "        labelsIndexRE = labelsIndex.reshape([batch_size*window_size]).cpu()\n",
    "        predictedRE = predicted.reshape([batch_size*window_size]).cpu()\n",
    "\n",
    "        # Adapt input and target for the cross entropy calculation\n",
    "        output = output.permute([0,2,1])\n",
    "        labels = labels.permute([0,2,1])\n",
    "\n",
    "        train_loss = loss(output, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += train_loss.item()\n",
    "        total += batch_size*window_size\n",
    "        running_accuracy += (predicted == labelsIndex).sum().item()/(batch_size*window_size)\n",
    "        running_weighted_f1 += f1_score(labelsIndexRE, predictedRE, average='weighted')\n",
    "        running_macro_f1 += f1_score(labelsIndexRE, predictedRE, average='macro')\n",
    "\n",
    "        sys.stdout.write(\"\\r{0}\".format(\"=\"*int(np.ceil(i/4)) + \"{0}>\".format(\"-\"*int(np.floor(len(gen)/4)-np.ceil(i/4)))) +\n",
    "            \"    {0}\".format(str(i).zfill(3)) + \n",
    "            \"    {:.5f}\".format(running_loss/(i+1)) + \"    {:.5f}\".format(running_accuracy/(i+1)) + \"    {:.5f}\".format(running_weighted_f1/(i+1)) + \"    {:.5f}\".format(running_macro_f1/(i+1)))\n",
    "        sys.stdout.flush()\n",
    "    return (running_accuracy/len(gen), running_loss/len(gen), running_weighted_f1/len(gen), running_macro_f1/len(gen), total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(gen,epoch_index):\n",
    "    loss = CrossEntropyLoss()\n",
    "    running_accuracy = 0.0 \n",
    "    running_vall_loss = 0.0\n",
    "    running_weighted_f1 = 0.0\n",
    "    running_macro_f1 = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad(): \n",
    "        model.eval() \n",
    "        for i, batch in enumerate(gen):\n",
    "            (input, labels) = batch\n",
    "            batch_size = labels.size(0)\n",
    "            window_size = labels.size(1)\n",
    "            number_classes = labels.size(2)\n",
    "            output = model(input)\n",
    "            \n",
    "            # # The label with the highest value will be our prediction\n",
    "            _, predicted = torch.max(output, 2)\n",
    "            _, labelsIndex = torch.max(labels, 2)\n",
    "\n",
    "            labelsIndexRE = labelsIndex.reshape([batch_size*window_size]).cpu()\n",
    "            predictedRE = predicted.reshape([batch_size*window_size]).cpu()\n",
    "\n",
    "            # Adapt input and target for the cross entropy calculation\n",
    "            output = output.permute([0,2,1])\n",
    "            labels = labels.permute([0,2,1])\n",
    "            eval_loss = loss(output, labels)\n",
    "            \n",
    "            running_vall_loss += eval_loss.item()\n",
    "            total += batch_size*window_size\n",
    "            running_accuracy += (predicted == labelsIndex).sum().item()/(batch_size*window_size)\n",
    "            running_weighted_f1 += f1_score(labelsIndexRE, predictedRE, average='weighted')\n",
    "            running_macro_f1 += f1_score(labelsIndexRE, predictedRE, average='macro')\n",
    "\n",
    "            sys.stdout.write(\"\\r{0}\".format(\"=\"*int(np.ceil(i/4)) + \"{0}>\".format(\"-\"*int(np.floor(len(gen)/4)-np.ceil(i/4)))) +\n",
    "            \"    {0}\".format(str(i).zfill(3)) + \n",
    "            \"    {:.5f}\".format(running_vall_loss/(i+1)) + \"    {:.5f}\".format(running_accuracy/(i+1)) + \"    {:.5f}\".format(running_weighted_f1/(i+1)) + \"    {:.5f}\".format(running_macro_f1/(i+1)))\n",
    "            sys.stdout.flush()\n",
    "    return (running_accuracy/len(gen), running_vall_loss/len(gen), running_weighted_f1/len(gen), running_macro_f1/len(gen), total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_logs = os.path.abspath(params['DIRECTORIES']['logs'])\n",
    "checkpoint_path=  path_logs + \"/ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUCell(128, 128)\n",
      "GRUCell(128, 128)\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=15, bias=True)\n",
      "  (7): Softmax(dim=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for el in model.children():\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 196\n",
      "===========================================================>    236    1.90177    0.91505    0.89682    0.55427\n",
      "Validating epoch 196\n",
      "===============>    058    1.89061    0.92629    0.90752    0.59384\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0\n",
      "         This LR: 0.0009979995670931533\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91120 - Weighted F1: 0.89305 - Macro F1: 0.55194\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91085 - Weighted F1: 0.89239 - Macro F1: 0.58394\n",
      "\n",
      "Training epoch 197\n",
      "===========================================================>    236    1.89313    0.92368    0.90512    0.55966\n",
      "Validating epoch 197\n",
      "===============>    058    1.88623    0.93064    0.91157    0.59830\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979995670931533\n",
      "         This LR: 0.0009979893710702096\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91980 - Weighted F1: 0.90132 - Macro F1: 0.55731\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91513 - Weighted F1: 0.89638 - Macro F1: 0.58833\n",
      "\n",
      "Training epoch 198\n",
      "===========================================================>    236    1.89043    0.92639    0.90750    0.56156\n",
      "Validating epoch 198\n",
      "===============>    058    1.88549    0.93142    0.91248    0.59842\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979893710702096\n",
      "         This LR: 0.0009979791751514333\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92249 - Weighted F1: 0.90368 - Macro F1: 0.55920\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91590 - Weighted F1: 0.89727 - Macro F1: 0.58844\n",
      "\n",
      "Training epoch 199\n",
      "===========================================================>    236    1.89124    0.92558    0.90686    0.56252\n",
      "Validating epoch 199\n",
      "===============>    058    1.88640    0.93051    0.91166    0.59792\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979791751514333\n",
      "         This LR: 0.0009979689793368233\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92169 - Weighted F1: 0.90305 - Macro F1: 0.56015\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91500 - Weighted F1: 0.89646 - Macro F1: 0.58796\n",
      "\n",
      "Training epoch 200\n",
      "===========================================================>    236    1.89390    0.92292    0.90359    0.55910\n",
      "Validating epoch 200\n",
      "===============>    058    1.88752    0.92939    0.91088    0.59671\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979689793368233\n",
      "         This LR: 0.0009979587836263782\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91904 - Weighted F1: 0.89979 - Macro F1: 0.55675\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91390 - Weighted F1: 0.89570 - Macro F1: 0.58677\n",
      "\n",
      "Training epoch 201\n",
      "===========================================================>    236    1.88975    0.92707    0.90840    0.56418\n",
      "Validating epoch 201\n",
      "===============>    058    1.89575    0.92116    0.90232    0.59180\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979587836263782\n",
      "         This LR: 0.0009979485880200973\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92317 - Weighted F1: 0.90459 - Macro F1: 0.56181\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.90581 - Weighted F1: 0.88728 - Macro F1: 0.58194\n",
      "\n",
      "Training epoch 202\n",
      "===========================================================>    236    1.90454    0.91227    0.89488    0.55504\n",
      "Validating epoch 202\n",
      "===============>    058    1.88514    0.93177    0.91354    0.60029\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979485880200973\n",
      "         This LR: 0.0009979383925179796\n",
      "             TRAIN ----- Loss: 1.90 - Accuracy: 0.90844 - Weighted F1: 0.89112 - Macro F1: 0.55271\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91624 - Weighted F1: 0.89832 - Macro F1: 0.59029\n",
      "\n",
      "Training epoch 203\n",
      "===========================================================>    236    1.88667    0.93015    0.91173    0.56783\n",
      "Validating epoch 203\n",
      "===============>    058    1.87892    0.93799    0.91943    0.60384\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979383925179796\n",
      "         This LR: 0.0009979281971200236\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92624 - Weighted F1: 0.90790 - Macro F1: 0.56544\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92235 - Weighted F1: 0.90411 - Macro F1: 0.59377\n",
      "\n",
      "Training epoch 204\n",
      "===========================================================>    236    1.88329    0.93353    0.91484    0.56979\n",
      "Validating epoch 204\n",
      "===============>    058    1.90686    0.91004    0.89225    0.58813\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979281971200236\n",
      "         This LR: 0.0009979180018262284\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92961 - Weighted F1: 0.91099 - Macro F1: 0.56740\n",
      "             EVAL  ----- Loss: 1.88 - Accuracy: 0.89488 - Weighted F1: 0.87738 - Macro F1: 0.57832\n",
      "\n",
      "Training epoch 205\n",
      "===========================================================>    236    1.88839    0.92843    0.90960    0.56465\n",
      "Validating epoch 205\n",
      "===============>    058    1.89312    0.92380    0.90511    0.59203\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979180018262284\n",
      "         This LR: 0.0009979078066365932\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92453 - Weighted F1: 0.90578 - Macro F1: 0.56228\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.90840 - Weighted F1: 0.89002 - Macro F1: 0.58216\n",
      "\n",
      "Training epoch 206\n",
      "===========================================================>    236    1.88981    0.92700    0.90814    0.56311\n",
      "Validating epoch 206\n",
      "===============>    058    1.88664    0.93028    0.91157    0.59742\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009979078066365932\n",
      "         This LR: 0.0009978976115511166\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92311 - Weighted F1: 0.90433 - Macro F1: 0.56075\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91478 - Weighted F1: 0.89638 - Macro F1: 0.58747\n",
      "\n",
      "Training epoch 207\n",
      "===========================================================>    236    1.88863    0.92817    0.90941    0.56406\n",
      "Validating epoch 207\n",
      "===============>    058    1.89008    0.92683    0.90842    0.59461\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978976115511166\n",
      "         This LR: 0.0009978874165697978\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92427 - Weighted F1: 0.90559 - Macro F1: 0.56169\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91139 - Weighted F1: 0.89328 - Macro F1: 0.58470\n",
      "\n",
      "Training epoch 208\n",
      "===========================================================>    236    1.88807    0.92875    0.91003    0.56383\n",
      "Validating epoch 208\n",
      "===============>    058    1.88919    0.92771    0.90913    0.59695\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978874165697978\n",
      "         This LR: 0.0009978772216926355\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92485 - Weighted F1: 0.90621 - Macro F1: 0.56146\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91225 - Weighted F1: 0.89398 - Macro F1: 0.58700\n",
      "\n",
      "Training epoch 209\n",
      "===========================================================>    236    1.88639    0.93042    0.91148    0.56535\n",
      "Validating epoch 209\n",
      "===============>    058    1.88907    0.92785    0.90924    0.59577\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978772216926355\n",
      "         This LR: 0.0009978670269196288\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92651 - Weighted F1: 0.90765 - Macro F1: 0.56297\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91239 - Weighted F1: 0.89409 - Macro F1: 0.58584\n",
      "\n",
      "Training epoch 210\n",
      "===========================================================>    236    1.90140    0.91541    0.89930    0.55391\n",
      "Validating epoch 210\n",
      "===============>    058    1.92026    0.89664    0.88509    0.57185\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978670269196288\n",
      "         This LR: 0.0009978568322507766\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91156 - Weighted F1: 0.89552 - Macro F1: 0.55158\n",
      "             EVAL  ----- Loss: 1.89 - Accuracy: 0.88170 - Weighted F1: 0.87034 - Macro F1: 0.56232\n",
      "\n",
      "Training epoch 211\n",
      "===========================================================>    236    1.90105    0.91576    0.89905    0.55440\n",
      "Validating epoch 211\n",
      "===============>    058    1.95629    0.86063    0.85216    0.54737\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978568322507766\n",
      "         This LR: 0.000997846637686078\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91191 - Weighted F1: 0.89527 - Macro F1: 0.55207\n",
      "             EVAL  ----- Loss: 1.92 - Accuracy: 0.84628 - Weighted F1: 0.83796 - Macro F1: 0.53825\n",
      "\n",
      "Training epoch 212\n",
      "===========================================================>    236    1.89530    0.92150    0.90327    0.55836\n",
      "Validating epoch 212\n",
      "===============>    058    1.88617    0.93073    0.91200    0.59618\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997846637686078\n",
      "         This LR: 0.0009978364432255316\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91763 - Weighted F1: 0.89947 - Macro F1: 0.55601\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91522 - Weighted F1: 0.89680 - Macro F1: 0.58625\n",
      "\n",
      "Training epoch 213\n",
      "===========================================================>    236    1.89108    0.92572    0.90718    0.56028\n",
      "Validating epoch 213\n",
      "===============>    058    1.88521    0.93169    0.91279    0.59716\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978364432255316\n",
      "         This LR: 0.0009978262488691365\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92183 - Weighted F1: 0.90337 - Macro F1: 0.55792\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91617 - Weighted F1: 0.89758 - Macro F1: 0.58721\n",
      "\n",
      "Training epoch 214\n",
      "===========================================================>    236    1.89158    0.92522    0.90659    0.56102\n",
      "Validating epoch 214\n",
      "===============>    058    1.88923    0.92767    0.90871    0.59386\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978262488691365\n",
      "         This LR: 0.0009978160546168916\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92134 - Weighted F1: 0.90278 - Macro F1: 0.55867\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91221 - Weighted F1: 0.89357 - Macro F1: 0.58396\n",
      "\n",
      "Training epoch 215\n",
      "===========================================================>    236    1.88866    0.92816    0.90942    0.56345\n",
      "Validating epoch 215\n",
      "===============>    058    1.88457    0.93235    0.91338    0.59917\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009978160546168916\n",
      "         This LR: 0.000997805860468796\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92426 - Weighted F1: 0.90560 - Macro F1: 0.56108\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91681 - Weighted F1: 0.89816 - Macro F1: 0.58918\n",
      "\n",
      "Training epoch 216\n",
      "===========================================================>    236    1.88991    0.92690    0.90835    0.56214\n",
      "Validating epoch 216\n",
      "===============>    058    1.88906    0.92787    0.90917    0.59271\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997805860468796\n",
      "         This LR: 0.0009977956664248486\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92301 - Weighted F1: 0.90454 - Macro F1: 0.55978\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91241 - Weighted F1: 0.89402 - Macro F1: 0.58283\n",
      "\n",
      "Training epoch 217\n",
      "===========================================================>    236    1.88694    0.92988    0.91108    0.56462\n",
      "Validating epoch 217\n",
      "===============>    058    1.88420    0.93271    0.91399    0.59761\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009977956664248486\n",
      "         This LR: 0.000997785472485048\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92597 - Weighted F1: 0.90726 - Macro F1: 0.56225\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91716 - Weighted F1: 0.89876 - Macro F1: 0.58765\n",
      "\n",
      "Training epoch 218\n",
      "===========================================================>    236    1.88756    0.92925    0.91055    0.56348\n",
      "Validating epoch 218\n",
      "===============>    058    1.88602    0.93088    0.91233    0.59490\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997785472485048\n",
      "         This LR: 0.0009977752786493935\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92534 - Weighted F1: 0.90672 - Macro F1: 0.56112\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91537 - Weighted F1: 0.89712 - Macro F1: 0.58498\n",
      "\n",
      "Training epoch 219\n",
      "===========================================================>    236    1.88669    0.93013    0.91146    0.56323\n",
      "Validating epoch 219\n",
      "===============>    058    1.87992    0.93698    0.91827    0.60183\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009977752786493935\n",
      "         This LR: 0.0009977650849178838\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92622 - Weighted F1: 0.90763 - Macro F1: 0.56086\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92136 - Weighted F1: 0.90296 - Macro F1: 0.59180\n",
      "\n",
      "Training epoch 220\n",
      "===========================================================>    236    1.88887    0.92794    0.90976    0.56342\n",
      "Validating epoch 220\n",
      "===============>    058    1.87786    0.93905    0.92042    0.60343\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009977650849178838\n",
      "         This LR: 0.0009977548912905182\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92404 - Weighted F1: 0.90594 - Macro F1: 0.56105\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92340 - Weighted F1: 0.90508 - Macro F1: 0.59337\n",
      "\n",
      "Training epoch 221\n",
      "===========================================================>    236    1.89003    0.92678    0.90893    0.56242\n",
      "Validating epoch 221\n",
      "===============>    058    1.88186    0.93505    0.91630    0.60104\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009977548912905182\n",
      "         This LR: 0.0009977446977672952\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92289 - Weighted F1: 0.90511 - Macro F1: 0.56006\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91946 - Weighted F1: 0.90103 - Macro F1: 0.59102\n",
      "\n",
      "Training epoch 222\n",
      "===========================================================>    236    1.88557    0.93124    0.91243    0.56542\n",
      "Validating epoch 222\n",
      "===============>    058    1.88400    0.93290    0.91417    0.59967\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009977446977672952\n",
      "         This LR: 0.000997734504348214\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92732 - Weighted F1: 0.90860 - Macro F1: 0.56305\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91735 - Weighted F1: 0.89893 - Macro F1: 0.58968\n",
      "\n",
      "Training epoch 223\n",
      "===========================================================>    236    1.90944    0.90737    0.89142    0.54472\n",
      "Validating epoch 223\n",
      "===============>    058    1.88326    0.93365    0.91519    0.59864\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997734504348214\n",
      "         This LR: 0.0009977243110332733\n",
      "             TRAIN ----- Loss: 1.90 - Accuracy: 0.90355 - Weighted F1: 0.88767 - Macro F1: 0.54243\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91809 - Weighted F1: 0.89993 - Macro F1: 0.58867\n",
      "\n",
      "Training epoch 224\n",
      "===========================================================>    236    1.92517    0.89163    0.87746    0.53330\n",
      "Validating epoch 224\n",
      "===============>    058    1.87992    0.93699    0.91829    0.60169\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009977243110332733\n",
      "         This LR: 0.0009977141178224723\n",
      "             TRAIN ----- Loss: 1.92 - Accuracy: 0.88788 - Weighted F1: 0.87378 - Macro F1: 0.53106\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92137 - Weighted F1: 0.90298 - Macro F1: 0.59166\n",
      "\n",
      "Training epoch 225\n",
      "===========================================================>    236    1.94549    0.87132    0.85785    0.51730\n",
      "Validating epoch 225\n",
      "===============>    058    2.17286    0.64405    0.65911    0.40808\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009977141178224723\n",
      "         This LR: 0.00099770392471581\n",
      "             TRAIN ----- Loss: 1.94 - Accuracy: 0.86766 - Weighted F1: 0.85424 - Macro F1: 0.51513\n",
      "             EVAL  ----- Loss: 2.14 - Accuracy: 0.63331 - Weighted F1: 0.64812 - Macro F1: 0.40128\n",
      "\n",
      "Training epoch 226\n",
      "===========================================================>    236    2.02032    0.79648    0.78630    0.46681\n",
      "Validating epoch 226\n",
      "===============>    058    1.96932    0.84758    0.84276    0.54013\n",
      "     Epoch stats:\n",
      "         Last LR: 0.00099770392471581\n",
      "         This LR: 0.000997693731713285\n",
      "             TRAIN ----- Loss: 2.01 - Accuracy: 0.79314 - Weighted F1: 0.78299 - Macro F1: 0.46485\n",
      "             EVAL  ----- Loss: 1.94 - Accuracy: 0.83345 - Weighted F1: 0.82871 - Macro F1: 0.53112\n",
      "\n",
      "Training epoch 227\n",
      "===========================================================>    236    1.90761    0.90920    0.89266    0.54829\n",
      "Validating epoch 227\n",
      "===============>    058    2.00259    0.81433    0.81617    0.52560\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997693731713285\n",
      "         This LR: 0.0009976835388148964\n",
      "             TRAIN ----- Loss: 1.90 - Accuracy: 0.90538 - Weighted F1: 0.88891 - Macro F1: 0.54598\n",
      "             EVAL  ----- Loss: 1.97 - Accuracy: 0.80076 - Weighted F1: 0.80256 - Macro F1: 0.51684\n",
      "\n",
      "Training epoch 228\n",
      "===========================================================>    236    1.92283    0.89398    0.87934    0.53891\n",
      "Validating epoch 228\n",
      "===============>    058    1.95198    0.86494    0.85593    0.55130\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976835388148964\n",
      "         This LR: 0.000997673346020643\n",
      "             TRAIN ----- Loss: 1.91 - Accuracy: 0.89023 - Weighted F1: 0.87564 - Macro F1: 0.53664\n",
      "             EVAL  ----- Loss: 1.92 - Accuracy: 0.85053 - Weighted F1: 0.84167 - Macro F1: 0.54211\n",
      "\n",
      "Training epoch 229\n",
      "===========================================================>    236    1.90565    0.91116    0.89434    0.55019\n",
      "Validating epoch 229\n",
      "===============>    058    1.95177    0.86513    0.85759    0.55112\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997673346020643\n",
      "         This LR: 0.0009976631533305243\n",
      "             TRAIN ----- Loss: 1.90 - Accuracy: 0.90733 - Weighted F1: 0.89058 - Macro F1: 0.54788\n",
      "             EVAL  ----- Loss: 1.92 - Accuracy: 0.85071 - Weighted F1: 0.84329 - Macro F1: 0.54193\n",
      "\n",
      "Training epoch 230\n",
      "===========================================================>    236    1.90773    0.90907    0.89266    0.54844\n",
      "Validating epoch 230\n",
      "===============>    058    1.97569    0.84122    0.83513    0.53454\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976631533305243\n",
      "         This LR: 0.0009976529607445385\n",
      "             TRAIN ----- Loss: 1.90 - Accuracy: 0.90525 - Weighted F1: 0.88891 - Macro F1: 0.54614\n",
      "             EVAL  ----- Loss: 1.94 - Accuracy: 0.82720 - Weighted F1: 0.82121 - Macro F1: 0.52563\n",
      "\n",
      "Training epoch 231\n",
      "===========================================================>    236    1.90016    0.91666    0.89969    0.55455\n",
      "Validating epoch 231\n",
      "===============>    058    1.88056    0.93635    0.91778    0.60095\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976529607445385\n",
      "         This LR: 0.0009976427682626848\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91281 - Weighted F1: 0.89591 - Macro F1: 0.55222\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92074 - Weighted F1: 0.90249 - Macro F1: 0.59094\n",
      "\n",
      "Training epoch 232\n",
      "===========================================================>    236    1.88998    0.92683    0.90882    0.56239\n",
      "Validating epoch 232\n",
      "===============>    058    1.87694    0.93996    0.92113    0.60473\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976427682626848\n",
      "         This LR: 0.0009976325758849624\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92294 - Weighted F1: 0.90500 - Macro F1: 0.56003\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92430 - Weighted F1: 0.90578 - Macro F1: 0.59465\n",
      "\n",
      "Training epoch 233\n",
      "===========================================================>    236    1.89274    0.92406    0.90639    0.55987\n",
      "Validating epoch 233\n",
      "===============>    058    1.87993    0.93698    0.91834    0.60181\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976325758849624\n",
      "         This LR: 0.0009976223836113698\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92018 - Weighted F1: 0.90258 - Macro F1: 0.55751\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92136 - Weighted F1: 0.90304 - Macro F1: 0.59178\n",
      "\n",
      "Training epoch 234\n",
      "===========================================================>    236    1.89033    0.92648    0.90862    0.56319\n",
      "Validating epoch 234\n",
      "===============>    058    1.88187    0.93504    0.91646    0.60000\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976223836113698\n",
      "         This LR: 0.0009976121914419063\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92259 - Weighted F1: 0.90481 - Macro F1: 0.56082\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91946 - Weighted F1: 0.90119 - Macro F1: 0.59000\n",
      "\n",
      "Training epoch 235\n",
      "===========================================================>    236    1.88801    0.92880    0.91059    0.56413\n",
      "Validating epoch 235\n",
      "===============>    058    1.89373    0.92318    0.90436    0.59423\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976121914419063\n",
      "         This LR: 0.0009976019993765707\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92490 - Weighted F1: 0.90677 - Macro F1: 0.56176\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.90779 - Weighted F1: 0.88929 - Macro F1: 0.58433\n",
      "\n",
      "Training epoch 236\n",
      "===========================================================>    236    1.88647    0.93034    0.91200    0.56517\n",
      "Validating epoch 236\n",
      "===============>    058    1.87774    0.93918    0.92039    0.60380\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009976019993765707\n",
      "         This LR: 0.000997591807415362\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92643 - Weighted F1: 0.90817 - Macro F1: 0.56279\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92353 - Weighted F1: 0.90505 - Macro F1: 0.59374\n",
      "\n",
      "Training epoch 237\n",
      "===========================================================>    236    1.88547    0.93134    0.91287    0.56652\n",
      "Validating epoch 237\n",
      "===============>    058    1.87755    0.93936    0.92056    0.60387\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997591807415362\n",
      "         This LR: 0.0009975816155582788\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92743 - Weighted F1: 0.90904 - Macro F1: 0.56414\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92370 - Weighted F1: 0.90522 - Macro F1: 0.59380\n",
      "\n",
      "Training epoch 238\n",
      "===========================================================>    236    1.88493    0.93188    0.91341    0.56609\n",
      "Validating epoch 238\n",
      "===============>    058    1.88923    0.92768    0.90897    0.59557\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975816155582788\n",
      "         This LR: 0.0009975714238053205\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92796 - Weighted F1: 0.90957 - Macro F1: 0.56371\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91222 - Weighted F1: 0.89382 - Macro F1: 0.58565\n",
      "\n",
      "Training epoch 239\n",
      "===========================================================>    236    1.88637    0.93044    0.91165    0.56434\n",
      "Validating epoch 239\n",
      "===============>    058    1.90729    0.90963    0.89093    0.58540\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975714238053205\n",
      "         This LR: 0.0009975612321564858\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92653 - Weighted F1: 0.90782 - Macro F1: 0.56197\n",
      "             EVAL  ----- Loss: 1.88 - Accuracy: 0.89447 - Weighted F1: 0.87609 - Macro F1: 0.57565\n",
      "\n",
      "Training epoch 240\n",
      "===========================================================>    236    1.93996    0.87684    0.85193    0.52402\n",
      "Validating epoch 240\n",
      "===============>    058    1.95379    0.86313    0.83860    0.54203\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975612321564858\n",
      "         This LR: 0.0009975510406117739\n",
      "             TRAIN ----- Loss: 1.93 - Accuracy: 0.87316 - Weighted F1: 0.84835 - Macro F1: 0.52181\n",
      "             EVAL  ----- Loss: 1.92 - Accuracy: 0.84874 - Weighted F1: 0.82463 - Macro F1: 0.53300\n",
      "\n",
      "Training epoch 241\n",
      "===========================================================>    236    1.89727    0.91954    0.90111    0.55544\n",
      "Validating epoch 241\n",
      "===============>    058    1.87951    0.93742    0.91862    0.60115\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975510406117739\n",
      "         This LR: 0.0009975408491711831\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91567 - Weighted F1: 0.89733 - Macro F1: 0.55311\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92179 - Weighted F1: 0.90331 - Macro F1: 0.59113\n",
      "\n",
      "Training epoch 242\n",
      "===========================================================>    236    1.88686    0.92994    0.91125    0.56464\n",
      "Validating epoch 242\n",
      "===============>    058    1.88034    0.93657    0.91768    0.60041\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975408491711831\n",
      "         This LR: 0.000997530657834713\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92604 - Weighted F1: 0.90742 - Macro F1: 0.56226\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92096 - Weighted F1: 0.90239 - Macro F1: 0.59040\n",
      "\n",
      "Training epoch 243\n",
      "===========================================================>    236    1.88655    0.93026    0.91165    0.56420\n",
      "Validating epoch 243\n",
      "===============>    058    1.88153    0.93538    0.91662    0.60013\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997530657834713\n",
      "         This LR: 0.0009975204666023622\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92635 - Weighted F1: 0.90782 - Macro F1: 0.56183\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91979 - Weighted F1: 0.90135 - Macro F1: 0.59012\n",
      "\n",
      "Training epoch 244\n",
      "===========================================================>    236    1.89210    0.92471    0.90632    0.56141\n",
      "Validating epoch 244\n",
      "===============>    058    1.87667    0.94024    0.92142    0.60384\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975204666023622\n",
      "         This LR: 0.0009975102754741297\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92082 - Weighted F1: 0.90251 - Macro F1: 0.55905\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92457 - Weighted F1: 0.90606 - Macro F1: 0.59377\n",
      "\n",
      "Training epoch 245\n",
      "===========================================================>    236    1.88459    0.93222    0.91347    0.56609\n",
      "Validating epoch 245\n",
      "===============>    058    1.88113    0.93578    0.91695    0.60142\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975102754741297\n",
      "         This LR: 0.0009975000844500146\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92830 - Weighted F1: 0.90963 - Macro F1: 0.56371\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92019 - Weighted F1: 0.90167 - Macro F1: 0.59139\n",
      "\n",
      "Training epoch 246\n",
      "===========================================================>    236    1.88757    0.92923    0.91050    0.56547\n",
      "Validating epoch 246\n",
      "===============>    058    1.88446    0.93245    0.91390    0.60030\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009975000844500146\n",
      "         This LR: 0.0009974898935300157\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92533 - Weighted F1: 0.90667 - Macro F1: 0.56309\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91691 - Weighted F1: 0.89867 - Macro F1: 0.59029\n",
      "\n",
      "Training epoch 247\n",
      "===========================================================>    236    1.88573    0.93109    0.91249    0.56617\n",
      "Validating epoch 247\n",
      "===============>    058    1.87975    0.93716    0.91844    0.60247\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009974898935300157\n",
      "         This LR: 0.0009974797027141318\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92718 - Weighted F1: 0.90865 - Macro F1: 0.56379\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92154 - Weighted F1: 0.90313 - Macro F1: 0.59243\n",
      "\n",
      "Training epoch 248\n",
      "===========================================================>    236    1.88641    0.93040    0.91231    0.56459\n",
      "Validating epoch 248\n",
      "===============>    058    1.89724    0.91966    0.90402    0.58726\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009974797027141318\n",
      "         This LR: 0.000997469512002362\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92649 - Weighted F1: 0.90848 - Macro F1: 0.56222\n",
      "             EVAL  ----- Loss: 1.87 - Accuracy: 0.90433 - Weighted F1: 0.88896 - Macro F1: 0.57748\n",
      "\n",
      "Training epoch 249\n",
      "===========================================================>    236    1.88799    0.92882    0.91078    0.56425\n",
      "Validating epoch 249\n",
      "===============>    058    1.88652    0.93039    0.91194    0.60024\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997469512002362\n",
      "         This LR: 0.0009974593213947051\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92491 - Weighted F1: 0.90695 - Macro F1: 0.56188\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91488 - Weighted F1: 0.89674 - Macro F1: 0.59024\n",
      "\n",
      "Training epoch 250\n",
      "===========================================================>    236    1.88453    0.93227    0.91367    0.56821\n",
      "Validating epoch 250\n",
      "===============>    058    1.88143    0.93548    0.91691    0.60384\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009974593213947051\n",
      "         This LR: 0.0009974491308911604\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92836 - Weighted F1: 0.90983 - Macro F1: 0.56583\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91989 - Weighted F1: 0.90163 - Macro F1: 0.59378\n",
      "\n",
      "Training epoch 251\n",
      "===========================================================>    236    1.88842    0.92839    0.91044    0.56461\n",
      "Validating epoch 251\n",
      "===============>    058    1.88124    0.93567    0.91707    0.60248\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009974491308911604\n",
      "         This LR: 0.0009974389404917264\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92448 - Weighted F1: 0.90662 - Macro F1: 0.56223\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92007 - Weighted F1: 0.90178 - Macro F1: 0.59244\n",
      "\n",
      "Training epoch 252\n",
      "===========================================================>    236    1.88399    0.93281    0.91403    0.56674\n",
      "Validating epoch 252\n",
      "===============>    058    1.87775    0.93916    0.92023    0.60369\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009974389404917264\n",
      "         This LR: 0.000997428750196402\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92889 - Weighted F1: 0.91019 - Macro F1: 0.56436\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92351 - Weighted F1: 0.90490 - Macro F1: 0.59363\n",
      "\n",
      "Training epoch 253\n",
      "===========================================================>    236    1.88831    0.92849    0.91001    0.56355\n",
      "Validating epoch 253\n",
      "===============>    058    1.88123    0.93568    0.91702    0.60266\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997428750196402\n",
      "         This LR: 0.0009974185600051867\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92459 - Weighted F1: 0.90619 - Macro F1: 0.56118\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92008 - Weighted F1: 0.90173 - Macro F1: 0.59262\n",
      "\n",
      "Training epoch 254\n",
      "===========================================================>    236    1.88865    0.92816    0.90937    0.56409\n",
      "Validating epoch 254\n",
      "===============>    058    1.88511    0.93180    0.91312    0.60001\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009974185600051867\n",
      "         This LR: 0.000997408369918079\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92426 - Weighted F1: 0.90555 - Macro F1: 0.56172\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91627 - Weighted F1: 0.89790 - Macro F1: 0.59001\n",
      "\n",
      "Training epoch 255\n",
      "===========================================================>    236    1.88492    0.93188    0.91300    0.56778\n",
      "Validating epoch 255\n",
      "===============>    058    1.87922    0.93769    0.91881    0.60357\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997408369918079\n",
      "         This LR: 0.000997398179935078\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92797 - Weighted F1: 0.90916 - Macro F1: 0.56539\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92206 - Weighted F1: 0.90350 - Macro F1: 0.59351\n",
      "\n",
      "Training epoch 256\n",
      "===========================================================>    236    1.88837    0.92843    0.90980    0.56499\n",
      "Validating epoch 256\n",
      "===============>    058    1.87958    0.93732    0.91869    0.60452\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997398179935078\n",
      "         This LR: 0.0009973879900561824\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92453 - Weighted F1: 0.90598 - Macro F1: 0.56262\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92170 - Weighted F1: 0.90338 - Macro F1: 0.59445\n",
      "\n",
      "Training epoch 257\n",
      "===========================================================>    236    1.88686    0.92995    0.91143    0.56607\n",
      "Validating epoch 257\n",
      "===============>    058    1.87823    0.93868    0.91998    0.60352\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009973879900561824\n",
      "         This LR: 0.000997377800281391\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92604 - Weighted F1: 0.90760 - Macro F1: 0.56369\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92304 - Weighted F1: 0.90465 - Macro F1: 0.59346\n",
      "\n",
      "Training epoch 258\n",
      "===========================================================>    236    1.88538    0.93143    0.91258    0.56666\n",
      "Validating epoch 258\n",
      "===============>    058    1.89237    0.92453    0.90616    0.59639\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997377800281391\n",
      "         This LR: 0.0009973676106107036\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92751 - Weighted F1: 0.90875 - Macro F1: 0.56428\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.90913 - Weighted F1: 0.89105 - Macro F1: 0.58645\n",
      "\n",
      "Training epoch 259\n",
      "===========================================================>    236    1.88760    0.92919    0.91038    0.56374\n",
      "Validating epoch 259\n",
      "===============>    058    1.88357    0.93334    0.91449    0.60003\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009973676106107036\n",
      "         This LR: 0.0009973574210441183\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92529 - Weighted F1: 0.90656 - Macro F1: 0.56137\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91778 - Weighted F1: 0.89925 - Macro F1: 0.59003\n",
      "\n",
      "Training epoch 260\n",
      "===========================================================>    236    1.89425    0.92256    0.90413    0.55901\n",
      "Validating epoch 260\n",
      "===============>    058    1.88444    0.93246    0.91349    0.59683\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009973574210441183\n",
      "         This LR: 0.0009973472315816343\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91868 - Weighted F1: 0.90033 - Macro F1: 0.55666\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91692 - Weighted F1: 0.89827 - Macro F1: 0.58689\n",
      "\n",
      "Training epoch 261\n",
      "===========================================================>    236    1.89060    0.92620    0.90763    0.56245\n",
      "Validating epoch 261\n",
      "===============>    058    1.88605    0.93086    0.91230    0.59948\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009973472315816343\n",
      "         This LR: 0.0009973370422232506\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92231 - Weighted F1: 0.90382 - Macro F1: 0.56008\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.91535 - Weighted F1: 0.89710 - Macro F1: 0.58949\n",
      "\n",
      "Training epoch 262\n",
      "===========================================================>    236    1.88713    0.92968    0.91086    0.56556\n",
      "Validating epoch 262\n",
      "===============>    058    1.88735    0.92956    0.91062    0.59577\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009973370422232506\n",
      "         This LR: 0.000997326852968966\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92577 - Weighted F1: 0.90703 - Macro F1: 0.56318\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91406 - Weighted F1: 0.89545 - Macro F1: 0.58584\n",
      "\n",
      "Training epoch 263\n",
      "===========================================================>    236    1.88921    0.92759    0.90878    0.56377\n",
      "Validating epoch 263\n",
      "===============>    058    1.89032    0.92658    0.90763    0.59496\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997326852968966\n",
      "         This LR: 0.0009973166638187795\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92369 - Weighted F1: 0.90496 - Macro F1: 0.56140\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.91114 - Weighted F1: 0.89250 - Macro F1: 0.58505\n",
      "\n",
      "Training epoch 264\n",
      "===========================================================>    236    1.88979    0.92701    0.90854    0.56287\n",
      "Validating epoch 264\n",
      "===============>    058    1.89449    0.92242    0.90446    0.58974\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009973166638187795\n",
      "         This LR: 0.00099730647477269\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92312 - Weighted F1: 0.90472 - Macro F1: 0.56050\n",
      "             EVAL  ----- Loss: 1.86 - Accuracy: 0.90704 - Weighted F1: 0.88939 - Macro F1: 0.57991\n",
      "\n",
      "Training epoch 265\n",
      "===========================================================>    236    1.89133    0.92547    0.90735    0.55973\n",
      "Validating epoch 265\n",
      "===============>    058    1.87705    0.93986    0.92095    0.60458\n",
      "     Epoch stats:\n",
      "         Last LR: 0.00099730647477269\n",
      "         This LR: 0.0009972962858306967\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92158 - Weighted F1: 0.90354 - Macro F1: 0.55738\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92419 - Weighted F1: 0.90560 - Macro F1: 0.59450\n",
      "\n",
      "Training epoch 266\n",
      "===========================================================>    236    1.88853    0.92828    0.90998    0.56267\n",
      "Validating epoch 266\n",
      "===============>    058    1.87996    0.93695    0.91823    0.60514\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972962858306967\n",
      "         This LR: 0.0009972860969927983\n",
      "             TRAIN ----- Loss: 1.88 - Accuracy: 0.92438 - Weighted F1: 0.90616 - Macro F1: 0.56031\n",
      "             EVAL  ----- Loss: 1.85 - Accuracy: 0.92133 - Weighted F1: 0.90292 - Macro F1: 0.59506\n",
      "\n",
      "Training epoch 267\n",
      "===========================================================>    236    1.89819    0.91861    0.90061    0.55417\n",
      "Validating epoch 267\n",
      "===============>    058    1.99391    0.82300    0.81145    0.50623\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972860969927983\n",
      "         This LR: 0.0009972759082589936\n",
      "             TRAIN ----- Loss: 1.89 - Accuracy: 0.91475 - Weighted F1: 0.89683 - Macro F1: 0.55184\n",
      "             EVAL  ----- Loss: 1.96 - Accuracy: 0.80929 - Weighted F1: 0.79793 - Macro F1: 0.49780\n",
      "\n",
      "Training epoch 268\n",
      "===========================================================>    236    1.96536    0.85145    0.83900    0.50164\n",
      "Validating epoch 268\n",
      "===============>    058    1.98769    0.82922    0.81749    0.51027\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972759082589936\n",
      "         This LR: 0.0009972657196292817\n",
      "             TRAIN ----- Loss: 1.96 - Accuracy: 0.84787 - Weighted F1: 0.83548 - Macro F1: 0.49954\n",
      "             EVAL  ----- Loss: 1.95 - Accuracy: 0.81540 - Weighted F1: 0.80387 - Macro F1: 0.50177\n",
      "\n",
      "Training epoch 269\n",
      "===========================================================>    236    2.00181    0.81500    0.80328    0.47351\n",
      "Validating epoch 269\n",
      "===============>    058    2.07409    0.74281    0.72906    0.44856\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972657196292817\n",
      "         This LR: 0.0009972555311036615\n",
      "             TRAIN ----- Loss: 1.99 - Accuracy: 0.81157 - Weighted F1: 0.79991 - Macro F1: 0.47152\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73043 - Weighted F1: 0.71691 - Macro F1: 0.44108\n",
      "\n",
      "Training epoch 270\n",
      "===========================================================>    236    2.07405    0.74276    0.72318    0.42062\n",
      "Validating epoch 270\n",
      "===============>    058    2.08277    0.73415    0.71792    0.43880\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972555311036615\n",
      "         This LR: 0.0009972453426821321\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73964 - Weighted F1: 0.72014 - Macro F1: 0.41885\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72191 - Weighted F1: 0.70596 - Macro F1: 0.43148\n",
      "\n",
      "Training epoch 271\n",
      "===========================================================>    236    2.09056    0.72625    0.69864    0.40765\n",
      "Validating epoch 271\n",
      "===============>    058    2.11969    0.69722    0.65765    0.40643\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972453426821321\n",
      "         This LR: 0.0009972351543646924\n",
      "             TRAIN ----- Loss: 2.08 - Accuracy: 0.72319 - Weighted F1: 0.69571 - Macro F1: 0.40593\n",
      "             EVAL  ----- Loss: 2.08 - Accuracy: 0.68560 - Weighted F1: 0.64669 - Macro F1: 0.39966\n",
      "\n",
      "Training epoch 272\n",
      "===========================================================>    236    2.09483    0.72198    0.68787    0.40529\n",
      "Validating epoch 272\n",
      "===============>    058    2.11954    0.69737    0.65367    0.40485\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972351543646924\n",
      "         This LR: 0.0009972249661513411\n",
      "             TRAIN ----- Loss: 2.09 - Accuracy: 0.71894 - Weighted F1: 0.68498 - Macro F1: 0.40359\n",
      "             EVAL  ----- Loss: 2.08 - Accuracy: 0.68574 - Weighted F1: 0.64278 - Macro F1: 0.39810\n",
      "\n",
      "Training epoch 273\n",
      "===========================================================>    236    2.09285    0.72396    0.69038    0.40787\n",
      "Validating epoch 273\n",
      "===============>    058    2.12099    0.69591    0.65565    0.40469\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972249661513411\n",
      "         This LR: 0.0009972147780420771\n",
      "             TRAIN ----- Loss: 2.08 - Accuracy: 0.72092 - Weighted F1: 0.68748 - Macro F1: 0.40616\n",
      "             EVAL  ----- Loss: 2.09 - Accuracy: 0.68431 - Weighted F1: 0.64472 - Macro F1: 0.39795\n",
      "\n",
      "Training epoch 274\n",
      "===========================================================>    236    2.08813    0.72867    0.69914    0.41307\n",
      "Validating epoch 274\n",
      "===============>    058    2.08074    0.73617    0.70946    0.43596\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972147780420771\n",
      "         This LR: 0.0009972045900368998\n",
      "             TRAIN ----- Loss: 2.08 - Accuracy: 0.72561 - Weighted F1: 0.69620 - Macro F1: 0.41133\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72390 - Weighted F1: 0.69764 - Macro F1: 0.42869\n",
      "\n",
      "Training epoch 275\n",
      "===========================================================>    236    2.09685    0.71996    0.70017    0.41323\n",
      "Validating epoch 275\n",
      "===============>    058    2.11513    0.70178    0.68386    0.42416\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009972045900368998\n",
      "         This LR: 0.0009971944021358078\n",
      "             TRAIN ----- Loss: 2.09 - Accuracy: 0.71693 - Weighted F1: 0.69722 - Macro F1: 0.41149\n",
      "             EVAL  ----- Loss: 2.08 - Accuracy: 0.69009 - Weighted F1: 0.67247 - Macro F1: 0.41709\n",
      "\n",
      "Training epoch 276\n",
      "===========================================================>    236    2.10229    0.71452    0.68719    0.40525\n",
      "Validating epoch 276\n",
      "===============>    058    2.09235    0.72456    0.70065    0.43043\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971944021358078\n",
      "         This LR: 0.0009971842143388\n",
      "             TRAIN ----- Loss: 2.09 - Accuracy: 0.71152 - Weighted F1: 0.68431 - Macro F1: 0.40354\n",
      "             EVAL  ----- Loss: 2.06 - Accuracy: 0.71248 - Weighted F1: 0.68897 - Macro F1: 0.42326\n",
      "\n",
      "Training epoch 277\n",
      "===========================================================>    236    2.09366    0.72314    0.69526    0.40961\n",
      "Validating epoch 277\n",
      "===============>    058    2.06733    0.74958    0.73507    0.45239\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971842143388\n",
      "         This LR: 0.0009971740266458754\n",
      "             TRAIN ----- Loss: 2.08 - Accuracy: 0.72010 - Weighted F1: 0.69234 - Macro F1: 0.40788\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73709 - Weighted F1: 0.72282 - Macro F1: 0.44485\n",
      "\n",
      "Training epoch 278\n",
      "===========================================================>    236    2.08891    0.72790    0.70894    0.41556\n",
      "Validating epoch 278\n",
      "===============>    058    2.07691    0.73999    0.72714    0.44700\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971740266458754\n",
      "         This LR: 0.000997163839057033\n",
      "             TRAIN ----- Loss: 2.08 - Accuracy: 0.72484 - Weighted F1: 0.70596 - Macro F1: 0.41381\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72766 - Weighted F1: 0.71502 - Macro F1: 0.43955\n",
      "\n",
      "Training epoch 279\n",
      "===========================================================>    236    2.08204    0.73476    0.71640    0.41826\n",
      "Validating epoch 279\n",
      "===============>    058    2.07381    0.74310    0.73131    0.45041\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997163839057033\n",
      "         This LR: 0.0009971536515722718\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73167 - Weighted F1: 0.71339 - Macro F1: 0.41650\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73071 - Weighted F1: 0.71912 - Macro F1: 0.44290\n",
      "\n",
      "Training epoch 280\n",
      "===========================================================>    236    2.07332    0.74349    0.72760    0.42482\n",
      "Validating epoch 280\n",
      "===============>    058    2.07557    0.74133    0.72940    0.44925\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971536515722718\n",
      "         This LR: 0.0009971434641915906\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74036 - Weighted F1: 0.72454 - Macro F1: 0.42303\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72898 - Weighted F1: 0.71724 - Macro F1: 0.44177\n",
      "\n",
      "Training epoch 281\n",
      "===========================================================>    236    2.07303    0.74378    0.72759    0.42503\n",
      "Validating epoch 281\n",
      "===============>    058    2.08279    0.73412    0.72430    0.44648\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971434641915906\n",
      "         This LR: 0.0009971332769149882\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74065 - Weighted F1: 0.72454 - Macro F1: 0.42324\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72189 - Weighted F1: 0.71223 - Macro F1: 0.43904\n",
      "\n",
      "Training epoch 282\n",
      "===========================================================>    236    2.07198    0.74483    0.73104    0.42701\n",
      "Validating epoch 282\n",
      "===============>    058    2.07286    0.74404    0.73283    0.45140\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971332769149882\n",
      "         This LR: 0.0009971230897424638\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74170 - Weighted F1: 0.72797 - Macro F1: 0.42522\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73164 - Weighted F1: 0.72061 - Macro F1: 0.44388\n",
      "\n",
      "Training epoch 283\n",
      "===========================================================>    236    2.06744    0.74937    0.73411    0.43062\n",
      "Validating epoch 283\n",
      "===============>    058    2.07111    0.74579    0.73282    0.45136\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971230897424638\n",
      "         This LR: 0.0009971129026740161\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74622 - Weighted F1: 0.73103 - Macro F1: 0.42881\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73336 - Weighted F1: 0.72061 - Macro F1: 0.44384\n",
      "\n",
      "Training epoch 284\n",
      "===========================================================>    236    2.05767    0.75914    0.74495    0.43849\n",
      "Validating epoch 284\n",
      "===============>    058    2.08314    0.73377    0.71569    0.44000\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971129026740161\n",
      "         This LR: 0.0009971027157096445\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75595 - Weighted F1: 0.74182 - Macro F1: 0.43665\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72154 - Weighted F1: 0.70376 - Macro F1: 0.43267\n",
      "\n",
      "Training epoch 285\n",
      "===========================================================>    236    2.04406    0.77275    0.75915    0.44868\n",
      "Validating epoch 285\n",
      "===============>    058    2.07805    0.73886    0.72635    0.44550\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009971027157096445\n",
      "         This LR: 0.0009970925288493473\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76950 - Weighted F1: 0.75596 - Macro F1: 0.44680\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72655 - Weighted F1: 0.71425 - Macro F1: 0.43808\n",
      "\n",
      "Training epoch 286\n",
      "===========================================================>    236    2.04572    0.77109    0.76033    0.44704\n",
      "Validating epoch 286\n",
      "===============>    058    2.07432    0.74258    0.73015    0.44621\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009970925288493473\n",
      "         This LR: 0.000997082342093124\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76785 - Weighted F1: 0.75713 - Macro F1: 0.44516\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73020 - Weighted F1: 0.71798 - Macro F1: 0.43878\n",
      "\n",
      "Training epoch 287\n",
      "===========================================================>    236    2.04467    0.77214    0.76085    0.44743\n",
      "Validating epoch 287\n",
      "===============>    058    2.07817    0.73871    0.72645    0.44694\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997082342093124\n",
      "         This LR: 0.0009970721554409733\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76889 - Weighted F1: 0.75765 - Macro F1: 0.44555\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72640 - Weighted F1: 0.71434 - Macro F1: 0.43949\n",
      "\n",
      "Training epoch 288\n",
      "===========================================================>    236    2.03466    0.78215    0.77253    0.45526\n",
      "Validating epoch 288\n",
      "===============>    058    2.07568    0.74123    0.72803    0.44729\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009970721554409733\n",
      "         This LR: 0.0009970619688928941\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77886 - Weighted F1: 0.76929 - Macro F1: 0.45335\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72888 - Weighted F1: 0.71590 - Macro F1: 0.43983\n",
      "\n",
      "Training epoch 289\n",
      "===========================================================>    236    2.03990    0.77690    0.76536    0.45060\n",
      "Validating epoch 289\n",
      "===============>    058    2.07266    0.74425    0.73098    0.44882\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009970619688928941\n",
      "         This LR: 0.000997051782448885\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77364 - Weighted F1: 0.76214 - Macro F1: 0.44871\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73184 - Weighted F1: 0.71879 - Macro F1: 0.44134\n",
      "\n",
      "Training epoch 290\n",
      "===========================================================>    236    2.03849    0.77831    0.76831    0.45289\n",
      "Validating epoch 290\n",
      "===============>    058    2.07078    0.74613    0.73371    0.45135\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997051782448885\n",
      "         This LR: 0.0009970415961089458\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77504 - Weighted F1: 0.76508 - Macro F1: 0.45098\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73370 - Weighted F1: 0.72148 - Macro F1: 0.44383\n",
      "\n",
      "Training epoch 291\n",
      "===========================================================>    236    2.04703    0.76977    0.75765    0.44677\n",
      "Validating epoch 291\n",
      "===============>    058    2.07652    0.74039    0.72791    0.44699\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009970415961089458\n",
      "         This LR: 0.0009970314098730747\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76654 - Weighted F1: 0.75447 - Macro F1: 0.44490\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72805 - Weighted F1: 0.71578 - Macro F1: 0.43954\n",
      "\n",
      "Training epoch 292\n",
      "===========================================================>    236    2.03918    0.77763    0.76892    0.45268\n",
      "Validating epoch 292\n",
      "===============>    058    2.07805    0.73886    0.72528    0.44417\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009970314098730747\n",
      "         This LR: 0.000997021223741271\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77436 - Weighted F1: 0.76569 - Macro F1: 0.45078\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72655 - Weighted F1: 0.71320 - Macro F1: 0.43677\n",
      "\n",
      "Training epoch 293\n",
      "===========================================================>    236    2.04233    0.77448    0.76524    0.45005\n",
      "Validating epoch 293\n",
      "===============>    058    2.08154    0.73537    0.72316    0.44325\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997021223741271\n",
      "         This LR: 0.0009970110377135334\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77122 - Weighted F1: 0.76202 - Macro F1: 0.44816\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72311 - Weighted F1: 0.71111 - Macro F1: 0.43586\n",
      "\n",
      "Training epoch 294\n",
      "===========================================================>    236    2.04067    0.77614    0.76600    0.45035\n",
      "Validating epoch 294\n",
      "===============>    058    2.07189    0.74502    0.73204    0.44894\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009970110377135334\n",
      "         This LR: 0.000997000851789861\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77288 - Weighted F1: 0.76278 - Macro F1: 0.44845\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73260 - Weighted F1: 0.71984 - Macro F1: 0.44146\n",
      "\n",
      "Training epoch 295\n",
      "===========================================================>    236    2.03871    0.77810    0.77013    0.45410\n",
      "Validating epoch 295\n",
      "===============>    058    2.07234    0.74457    0.73267    0.45099\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000997000851789861\n",
      "         This LR: 0.0009969906659702526\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77483 - Weighted F1: 0.76690 - Macro F1: 0.45219\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73216 - Weighted F1: 0.72046 - Macro F1: 0.44347\n",
      "\n",
      "Training epoch 296\n",
      "===========================================================>    236    2.03980    0.77701    0.76852    0.45243\n",
      "Validating epoch 296\n",
      "===============>    058    2.07413    0.74278    0.73101    0.44971\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009969906659702526\n",
      "         This LR: 0.0009969804802547073\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77375 - Weighted F1: 0.76529 - Macro F1: 0.45053\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73040 - Weighted F1: 0.71882 - Macro F1: 0.44221\n",
      "\n",
      "Training epoch 297\n",
      "===========================================================>    236    2.04568    0.77113    0.76206    0.44850\n",
      "Validating epoch 297\n",
      "===============>    058    2.07660    0.74030    0.72856    0.44779\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009969804802547073\n",
      "         This LR: 0.000996970294643224\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76789 - Weighted F1: 0.75885 - Macro F1: 0.44662\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72796 - Weighted F1: 0.71642 - Macro F1: 0.44033\n",
      "\n",
      "Training epoch 298\n",
      "===========================================================>    236    2.04873    0.76807    0.75941    0.44580\n",
      "Validating epoch 298\n",
      "===============>    058    2.07659    0.74032    0.72850    0.44750\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996970294643224\n",
      "         This LR: 0.0009969601091358014\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76485 - Weighted F1: 0.75622 - Macro F1: 0.44393\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72798 - Weighted F1: 0.71635 - Macro F1: 0.44004\n",
      "\n",
      "Training epoch 299\n",
      "===========================================================>    236    2.05568    0.76112    0.75209    0.44092\n",
      "Validating epoch 299\n",
      "===============>    058    2.07832    0.73859    0.72670    0.44500\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009969601091358014\n",
      "         This LR: 0.0009969499237324389\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75792 - Weighted F1: 0.74893 - Macro F1: 0.43907\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72628 - Weighted F1: 0.71458 - Macro F1: 0.43758\n",
      "\n",
      "Training epoch 300\n",
      "===========================================================>    236    2.06412    0.75268    0.73963    0.43214\n",
      "Validating epoch 300\n",
      "===============>    058    2.08786    0.72905    0.70931    0.43329\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009969499237324389\n",
      "         This LR: 0.0009969397384331348\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74952 - Weighted F1: 0.73652 - Macro F1: 0.43033\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.71690 - Weighted F1: 0.69749 - Macro F1: 0.42607\n",
      "\n",
      "Training epoch 301\n",
      "===========================================================>    236    2.07148    0.74532    0.73590    0.42987\n",
      "Validating epoch 301\n",
      "===============>    058    2.08454    0.73237    0.71744    0.43880\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009969397384331348\n",
      "         This LR: 0.0009969295532378888\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74219 - Weighted F1: 0.73281 - Macro F1: 0.42806\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72016 - Weighted F1: 0.70548 - Macro F1: 0.43148\n",
      "\n",
      "Training epoch 302\n",
      "===========================================================>    236    2.06557    0.75123    0.73854    0.43177\n",
      "Validating epoch 302\n",
      "===============>    058    2.07559    0.74132    0.72788    0.44774\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009969295532378888\n",
      "         This LR: 0.000996919368146699\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74808 - Weighted F1: 0.73544 - Macro F1: 0.42995\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72896 - Weighted F1: 0.71575 - Macro F1: 0.44028\n",
      "\n",
      "Training epoch 303\n",
      "===========================================================>    236    2.06193    0.75488    0.74274    0.43402\n",
      "Validating epoch 303\n",
      "===============>    058    2.07803    0.73888    0.72454    0.44518\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996919368146699\n",
      "         This LR: 0.0009969091831595652\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75171 - Weighted F1: 0.73962 - Macro F1: 0.43219\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72656 - Weighted F1: 0.71246 - Macro F1: 0.43776\n",
      "\n",
      "Training epoch 304\n",
      "===========================================================>    236    2.05947    0.75733    0.74383    0.43525\n",
      "Validating epoch 304\n",
      "===============>    058    2.06956    0.74735    0.72950    0.44823\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009969091831595652\n",
      "         This LR: 0.0009968989982764857\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75415 - Weighted F1: 0.74071 - Macro F1: 0.43342\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73489 - Weighted F1: 0.71735 - Macro F1: 0.44076\n",
      "\n",
      "Training epoch 305\n",
      "===========================================================>    236    2.06324    0.75356    0.73469    0.43019\n",
      "Validating epoch 305\n",
      "===============>    058    2.07118    0.74573    0.72933    0.44808\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009968989982764857\n",
      "         This LR: 0.00099688881349746\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75039 - Weighted F1: 0.73160 - Macro F1: 0.42839\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73330 - Weighted F1: 0.71717 - Macro F1: 0.44061\n",
      "\n",
      "Training epoch 306\n",
      "===========================================================>    236    2.05890    0.75791    0.74136    0.43395\n",
      "Validating epoch 306\n",
      "===============>    058    2.06587    0.75104    0.73483    0.45057\n",
      "     Epoch stats:\n",
      "         Last LR: 0.00099688881349746\n",
      "         This LR: 0.0009968786288224864\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75473 - Weighted F1: 0.73825 - Macro F1: 0.43212\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73852 - Weighted F1: 0.72258 - Macro F1: 0.44306\n",
      "\n",
      "Training epoch 307\n",
      "===========================================================>    236    2.06213    0.75468    0.73572    0.43062\n",
      "Validating epoch 307\n",
      "===============>    058    2.06907    0.74784    0.72955    0.44762\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009968786288224864\n",
      "         This LR: 0.000996868444251564\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75151 - Weighted F1: 0.73263 - Macro F1: 0.42881\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73538 - Weighted F1: 0.71739 - Macro F1: 0.44016\n",
      "\n",
      "Training epoch 308\n",
      "===========================================================>    236    2.07144    0.74536    0.72563    0.42301\n",
      "Validating epoch 308\n",
      "===============>    058    2.08223    0.73468    0.71506    0.44058\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996868444251564\n",
      "         This LR: 0.000996858259784692\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74223 - Weighted F1: 0.72258 - Macro F1: 0.42124\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72243 - Weighted F1: 0.70314 - Macro F1: 0.43323\n",
      "\n",
      "Training epoch 309\n",
      "===========================================================>    236    2.06693    0.74988    0.73439    0.42829\n",
      "Validating epoch 309\n",
      "===============>    058    2.07541    0.74148    0.72716    0.44655\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996858259784692\n",
      "         This LR: 0.000996848075421869\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74673 - Weighted F1: 0.73130 - Macro F1: 0.42649\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72912 - Weighted F1: 0.71504 - Macro F1: 0.43911\n",
      "\n",
      "Training epoch 310\n",
      "===========================================================>    236    2.06588    0.75092    0.73463    0.42794\n",
      "Validating epoch 310\n",
      "===============>    058    2.07199    0.74492    0.73079    0.44821\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996848075421869\n",
      "         This LR: 0.0009968378911630945\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74777 - Weighted F1: 0.73155 - Macro F1: 0.42614\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73251 - Weighted F1: 0.71861 - Macro F1: 0.44074\n",
      "\n",
      "Training epoch 311\n",
      "===========================================================>    236    2.06427    0.75254    0.73445    0.42875\n",
      "Validating epoch 311\n",
      "===============>    058    2.06939    0.74752    0.73260    0.45007\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009968378911630945\n",
      "         This LR: 0.000996827707008367\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74938 - Weighted F1: 0.73137 - Macro F1: 0.42695\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73506 - Weighted F1: 0.72039 - Macro F1: 0.44257\n",
      "\n",
      "Training epoch 312\n",
      "===========================================================>    236    2.06252    0.75429    0.73691    0.42981\n",
      "Validating epoch 312\n",
      "===============>    058    2.06796    0.74895    0.73409    0.45134\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996827707008367\n",
      "         This LR: 0.0009968175229576853\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75112 - Weighted F1: 0.73382 - Macro F1: 0.42801\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73647 - Weighted F1: 0.72185 - Macro F1: 0.44381\n",
      "\n",
      "Training epoch 313\n",
      "===========================================================>    236    2.06301    0.75379    0.73539    0.42926\n",
      "Validating epoch 313\n",
      "===============>    058    2.06848    0.74842    0.73384    0.45053\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009968175229576853\n",
      "         This LR: 0.0009968073390110488\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75063 - Weighted F1: 0.73230 - Macro F1: 0.42746\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73595 - Weighted F1: 0.72161 - Macro F1: 0.44302\n",
      "\n",
      "Training epoch 314\n",
      "===========================================================>    236    2.06343    0.75337    0.73340    0.42861\n",
      "Validating epoch 314\n",
      "===============>    058    2.06678    0.75013    0.73595    0.45215\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009968073390110488\n",
      "         This LR: 0.000996797155168456\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75021 - Weighted F1: 0.73032 - Macro F1: 0.42681\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73763 - Weighted F1: 0.72368 - Macro F1: 0.44461\n",
      "\n",
      "Training epoch 315\n",
      "===========================================================>    236    2.06517    0.75164    0.73506    0.42962\n",
      "Validating epoch 315\n",
      "===============>    058    2.07749    0.73942    0.72629    0.44522\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996797155168456\n",
      "         This LR: 0.000996786971429906\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74848 - Weighted F1: 0.73197 - Macro F1: 0.42781\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72709 - Weighted F1: 0.71419 - Macro F1: 0.43780\n",
      "\n",
      "Training epoch 316\n",
      "===========================================================>    236    2.06483    0.75197    0.73487    0.42832\n",
      "Validating epoch 316\n",
      "===============>    058    2.07380    0.74310    0.73012    0.44776\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996786971429906\n",
      "         This LR: 0.0009967767877953981\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74882 - Weighted F1: 0.73179 - Macro F1: 0.42652\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73072 - Weighted F1: 0.71795 - Macro F1: 0.44030\n",
      "\n",
      "Training epoch 317\n",
      "===========================================================>    236    2.06293    0.75387    0.73630    0.43020\n",
      "Validating epoch 317\n",
      "===============>    058    2.06990    0.74700    0.73346    0.45045\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009967767877953981\n",
      "         This LR: 0.0009967666042649307\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75070 - Weighted F1: 0.73320 - Macro F1: 0.42840\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73455 - Weighted F1: 0.72123 - Macro F1: 0.44294\n",
      "\n",
      "Training epoch 318\n",
      "===========================================================>    236    2.06276    0.75405    0.73587    0.42978\n",
      "Validating epoch 318\n",
      "===============>    058    2.06993    0.74698    0.73308    0.44963\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009967666042649307\n",
      "         This LR: 0.0009967564208385026\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75088 - Weighted F1: 0.73278 - Macro F1: 0.42798\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73453 - Weighted F1: 0.72087 - Macro F1: 0.44214\n",
      "\n",
      "Training epoch 319\n",
      "===========================================================>    236    2.06377    0.75304    0.73498    0.43020\n",
      "Validating epoch 319\n",
      "===============>    058    2.06521    0.75168    0.73674    0.45229\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009967564208385026\n",
      "         This LR: 0.0009967462375161135\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74988 - Weighted F1: 0.73189 - Macro F1: 0.42839\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73915 - Weighted F1: 0.72446 - Macro F1: 0.44475\n",
      "\n",
      "Training epoch 320\n",
      "===========================================================>    236    2.06640    0.75041    0.73380    0.42947\n",
      "Validating epoch 320\n",
      "===============>    058    2.07150    0.74541    0.73013    0.44824\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009967462375161135\n",
      "         This LR: 0.0009967360542977616\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74725 - Weighted F1: 0.73072 - Macro F1: 0.42767\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73298 - Weighted F1: 0.71797 - Macro F1: 0.44077\n",
      "\n",
      "Training epoch 321\n",
      "===========================================================>    236    2.07324    0.74357    0.72479    0.42319\n",
      "Validating epoch 321\n",
      "===============>    058    2.09197    0.72494    0.69408    0.42544\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009967360542977616\n",
      "         This LR: 0.0009967258711834465\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74044 - Weighted F1: 0.72175 - Macro F1: 0.42141\n",
      "             EVAL  ----- Loss: 2.06 - Accuracy: 0.71286 - Weighted F1: 0.68251 - Macro F1: 0.41835\n",
      "\n",
      "Training epoch 322\n",
      "===========================================================>    236    2.08146    0.73535    0.71525    0.41772\n",
      "Validating epoch 322\n",
      "===============>    058    2.07849    0.73840    0.72691    0.44690\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009967258711834465\n",
      "         This LR: 0.0009967156881731667\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73226 - Weighted F1: 0.71224 - Macro F1: 0.41597\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72609 - Weighted F1: 0.71480 - Macro F1: 0.43945\n",
      "\n",
      "Training epoch 323\n",
      "===========================================================>    236    2.06473    0.75208    0.73243    0.42946\n",
      "Validating epoch 323\n",
      "===============>    058    2.07494    0.74198    0.73020    0.44903\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009967156881731667\n",
      "         This LR: 0.000996705505266921\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74892 - Weighted F1: 0.72936 - Macro F1: 0.42765\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72961 - Weighted F1: 0.71803 - Macro F1: 0.44154\n",
      "\n",
      "Training epoch 324\n",
      "===========================================================>    236    2.06346    0.75335    0.73725    0.43115\n",
      "Validating epoch 324\n",
      "===============>    058    2.07202    0.74489    0.73273    0.45048\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996705505266921\n",
      "         This LR: 0.0009966953224647087\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75018 - Weighted F1: 0.73415 - Macro F1: 0.42933\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73247 - Weighted F1: 0.72052 - Macro F1: 0.44297\n",
      "\n",
      "Training epoch 325\n",
      "===========================================================>    236    2.06635    0.75045    0.73578    0.43078\n",
      "Validating epoch 325\n",
      "===============>    058    2.07092    0.74599    0.73408    0.45128\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966953224647087\n",
      "         This LR: 0.0009966851397665286\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74730 - Weighted F1: 0.73269 - Macro F1: 0.42897\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73356 - Weighted F1: 0.72184 - Macro F1: 0.44376\n",
      "\n",
      "Training epoch 326\n",
      "===========================================================>    236    2.06495    0.75185    0.73514    0.43095\n",
      "Validating epoch 326\n",
      "===============>    058    2.06929    0.74762    0.73484    0.45173\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966851397665286\n",
      "         This LR: 0.0009966749571723795\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74869 - Weighted F1: 0.73205 - Macro F1: 0.42914\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73516 - Weighted F1: 0.72260 - Macro F1: 0.44420\n",
      "\n",
      "Training epoch 327\n",
      "===========================================================>    236    2.06647    0.75034    0.73357    0.42981\n",
      "Validating epoch 327\n",
      "===============>    058    2.07758    0.73933    0.72913    0.44873\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966749571723795\n",
      "         This LR: 0.0009966647746822606\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74719 - Weighted F1: 0.73049 - Macro F1: 0.42800\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72701 - Weighted F1: 0.71698 - Macro F1: 0.44125\n",
      "\n",
      "Training epoch 328\n",
      "===========================================================>    236    2.08166    0.73515    0.72052    0.42292\n",
      "Validating epoch 328\n",
      "===============>    058    2.10789    0.70902    0.70187    0.43444\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966647746822606\n",
      "         This LR: 0.0009966545922961707\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73206 - Weighted F1: 0.71749 - Macro F1: 0.42115\n",
      "             EVAL  ----- Loss: 2.07 - Accuracy: 0.69720 - Weighted F1: 0.69017 - Macro F1: 0.42719\n",
      "\n",
      "Training epoch 329\n",
      "===========================================================>    236    2.08960    0.72720    0.70952    0.41724\n",
      "Validating epoch 329\n",
      "===============>    058    2.08206    0.73485    0.71936    0.44329\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966545922961707\n",
      "         This LR: 0.0009966444100141087\n",
      "             TRAIN ----- Loss: 2.08 - Accuracy: 0.72415 - Weighted F1: 0.70653 - Macro F1: 0.41549\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72260 - Weighted F1: 0.70737 - Macro F1: 0.43590\n",
      "\n",
      "Training epoch 330\n",
      "===========================================================>    236    2.08201    0.73480    0.72221    0.42362\n",
      "Validating epoch 330\n",
      "===============>    058    2.08296    0.73395    0.72229    0.44560\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966444100141087\n",
      "         This LR: 0.0009966342278360735\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73171 - Weighted F1: 0.71918 - Macro F1: 0.42184\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72171 - Weighted F1: 0.71025 - Macro F1: 0.43818\n",
      "\n",
      "Training epoch 331\n",
      "===========================================================>    236    2.07835    0.73846    0.72460    0.42516\n",
      "Validating epoch 331\n",
      "===============>    058    2.07396    0.74294    0.73002    0.44987\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966342278360735\n",
      "         This LR: 0.0009966240457620643\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73536 - Weighted F1: 0.72156 - Macro F1: 0.42337\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73056 - Weighted F1: 0.71785 - Macro F1: 0.44237\n",
      "\n",
      "Training epoch 332\n",
      "===========================================================>    236    2.06783    0.74897    0.72710    0.42715\n",
      "Validating epoch 332\n",
      "===============>    058    2.07139    0.74552    0.72920    0.44970\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966240457620643\n",
      "         This LR: 0.0009966138637920797\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74583 - Weighted F1: 0.72404 - Macro F1: 0.42535\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73309 - Weighted F1: 0.71705 - Macro F1: 0.44220\n",
      "\n",
      "Training epoch 333\n",
      "===========================================================>    236    2.06517    0.75164    0.72802    0.42857\n",
      "Validating epoch 333\n",
      "===============>    058    2.06969    0.74721    0.73039    0.45044\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966138637920797\n",
      "         This LR: 0.0009966036819261189\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74848 - Weighted F1: 0.72496 - Macro F1: 0.42677\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73476 - Weighted F1: 0.71822 - Macro F1: 0.44293\n",
      "\n",
      "Training epoch 334\n",
      "===========================================================>    236    2.06604    0.75077    0.72187    0.42620\n",
      "Validating epoch 334\n",
      "===============>    058    2.07397    0.74294    0.72611    0.44700\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009966036819261189\n",
      "         This LR: 0.0009965935001641807\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74762 - Weighted F1: 0.71884 - Macro F1: 0.42441\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73056 - Weighted F1: 0.71401 - Macro F1: 0.43955\n",
      "\n",
      "Training epoch 335\n",
      "===========================================================>    236    2.06716    0.74965    0.72714    0.42728\n",
      "Validating epoch 335\n",
      "===============>    058    2.07275    0.74416    0.72719    0.44813\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009965935001641807\n",
      "         This LR: 0.000996583318506264\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74650 - Weighted F1: 0.72409 - Macro F1: 0.42548\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73176 - Weighted F1: 0.71507 - Macro F1: 0.44066\n",
      "\n",
      "Training epoch 336\n",
      "===========================================================>    236    2.06122    0.75558    0.72971    0.43038\n",
      "Validating epoch 336\n",
      "===============>    058    2.06773    0.74919    0.73417    0.45215\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996583318506264\n",
      "         This LR: 0.000996573136952368\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75241 - Weighted F1: 0.72665 - Macro F1: 0.42857\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73671 - Weighted F1: 0.72193 - Macro F1: 0.44461\n",
      "\n",
      "Training epoch 337\n",
      "===========================================================>    236    2.06518    0.75163    0.73539    0.43058\n",
      "Validating epoch 337\n",
      "===============>    058    2.07006    0.74684    0.72961    0.44917\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996573136952368\n",
      "         This LR: 0.0009965629555024912\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74847 - Weighted F1: 0.73230 - Macro F1: 0.42877\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73440 - Weighted F1: 0.71745 - Macro F1: 0.44169\n",
      "\n",
      "Training epoch 338\n",
      "===========================================================>    236    2.06805    0.74876    0.73065    0.42794\n",
      "Validating epoch 338\n",
      "===============>    058    2.07849    0.73843    0.71198    0.43750\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009965629555024912\n",
      "         This LR: 0.0009965527741566328\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74561 - Weighted F1: 0.72758 - Macro F1: 0.42614\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72612 - Weighted F1: 0.70011 - Macro F1: 0.43021\n",
      "\n",
      "Training epoch 339\n",
      "===========================================================>    236    2.06359    0.75322    0.73454    0.43014\n",
      "Validating epoch 339\n",
      "===============>    058    2.07079    0.74612    0.72766    0.44828\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009965527741566328\n",
      "         This LR: 0.000996542592914792\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75005 - Weighted F1: 0.73146 - Macro F1: 0.42833\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73369 - Weighted F1: 0.71554 - Macro F1: 0.44081\n",
      "\n",
      "Training epoch 340\n",
      "===========================================================>    236    2.06143    0.75538    0.73690    0.43121\n",
      "Validating epoch 340\n",
      "===============>    058    2.06512    0.75178    0.73492    0.45258\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996542592914792\n",
      "         This LR: 0.0009965324117769672\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75221 - Weighted F1: 0.73380 - Macro F1: 0.42939\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73926 - Weighted F1: 0.72267 - Macro F1: 0.44504\n",
      "\n",
      "Training epoch 341\n",
      "===========================================================>    236    2.06218    0.75462    0.73604    0.43115\n",
      "Validating epoch 341\n",
      "===============>    058    2.06474    0.75216    0.73297    0.45083\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009965324117769672\n",
      "         This LR: 0.0009965222307431576\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75145 - Weighted F1: 0.73295 - Macro F1: 0.42934\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73962 - Weighted F1: 0.72075 - Macro F1: 0.44332\n",
      "\n",
      "Training epoch 342\n",
      "===========================================================>    236    2.06758    0.74922    0.73401    0.43034\n",
      "Validating epoch 342\n",
      "===============>    058    2.06596    0.75095    0.73483    0.45237\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009965222307431576\n",
      "         This LR: 0.0009965120498133622\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74607 - Weighted F1: 0.73093 - Macro F1: 0.42853\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73843 - Weighted F1: 0.72258 - Macro F1: 0.44483\n",
      "\n",
      "Training epoch 343\n",
      "===========================================================>    236    2.06169    0.75511    0.73761    0.43216\n",
      "Validating epoch 343\n",
      "===============>    058    2.05991    0.75699    0.73805    0.45405\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009965120498133622\n",
      "         This LR: 0.0009965018689875797\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75194 - Weighted F1: 0.73451 - Macro F1: 0.43034\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74438 - Weighted F1: 0.72575 - Macro F1: 0.44648\n",
      "\n",
      "Training epoch 344\n",
      "===========================================================>    236    2.06212    0.75468    0.73666    0.43193\n",
      "Validating epoch 344\n",
      "===============>    058    2.06950    0.74740    0.73309    0.45190\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009965018689875797\n",
      "         This LR: 0.0009964916882658093\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75151 - Weighted F1: 0.73357 - Macro F1: 0.43011\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73495 - Weighted F1: 0.72087 - Macro F1: 0.44436\n",
      "\n",
      "Training epoch 345\n",
      "===========================================================>    236    2.06396    0.75284    0.73359    0.42961\n",
      "Validating epoch 345\n",
      "===============>    058    2.06316    0.75376    0.73632    0.45404\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964916882658093\n",
      "         This LR: 0.00099648150764805\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74968 - Weighted F1: 0.73051 - Macro F1: 0.42781\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74119 - Weighted F1: 0.72405 - Macro F1: 0.44648\n",
      "\n",
      "Training epoch 346\n",
      "===========================================================>    236    2.07671    0.74009    0.72243    0.42313\n",
      "Validating epoch 346\n",
      "===============>    058    2.06863    0.74828    0.73355    0.45182\n",
      "     Epoch stats:\n",
      "         Last LR: 0.00099648150764805\n",
      "         This LR: 0.0009964713271343004\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73698 - Weighted F1: 0.71939 - Macro F1: 0.42135\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73581 - Weighted F1: 0.72132 - Macro F1: 0.44429\n",
      "\n",
      "Training epoch 347\n",
      "===========================================================>    236    2.06312    0.75368    0.73810    0.43255\n",
      "Validating epoch 347\n",
      "===============>    058    2.06795    0.74896    0.73466    0.45261\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964713271343004\n",
      "         This LR: 0.0009964611467245597\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75052 - Weighted F1: 0.73500 - Macro F1: 0.43073\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73647 - Weighted F1: 0.72241 - Macro F1: 0.44506\n",
      "\n",
      "Training epoch 348\n",
      "===========================================================>    236    2.06163    0.75518    0.73789    0.43333\n",
      "Validating epoch 348\n",
      "===============>    058    2.06771    0.74920    0.73252    0.45080\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964611467245597\n",
      "         This LR: 0.0009964509664188268\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75200 - Weighted F1: 0.73479 - Macro F1: 0.43151\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73671 - Weighted F1: 0.72031 - Macro F1: 0.44329\n",
      "\n",
      "Training epoch 349\n",
      "===========================================================>    236    2.05766    0.75915    0.74108    0.43533\n",
      "Validating epoch 349\n",
      "===============>    058    2.06437    0.75254    0.73747    0.45403\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964509664188268\n",
      "         This LR: 0.0009964407862171005\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75596 - Weighted F1: 0.73796 - Macro F1: 0.43350\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73999 - Weighted F1: 0.72518 - Macro F1: 0.44646\n",
      "\n",
      "Training epoch 350\n",
      "===========================================================>    236    2.05528    0.76152    0.74474    0.43706\n",
      "Validating epoch 350\n",
      "===============>    058    2.06267    0.75424    0.73739    0.45448\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964407862171005\n",
      "         This LR: 0.0009964306061193797\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75832 - Weighted F1: 0.74161 - Macro F1: 0.43523\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74167 - Weighted F1: 0.72510 - Macro F1: 0.44690\n",
      "\n",
      "Training epoch 351\n",
      "===========================================================>    236    2.05410    0.76270    0.74471    0.43748\n",
      "Validating epoch 351\n",
      "===============>    058    2.06293    0.75399    0.73897    0.45564\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964306061193797\n",
      "         This LR: 0.0009964204261256636\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75950 - Weighted F1: 0.74158 - Macro F1: 0.43564\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74142 - Weighted F1: 0.72665 - Macro F1: 0.44805\n",
      "\n",
      "Training epoch 352\n",
      "===========================================================>    236    2.05514    0.76167    0.74758    0.43826\n",
      "Validating epoch 352\n",
      "===============>    058    2.06314    0.75377    0.73927    0.45553\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964204261256636\n",
      "         This LR: 0.000996410246235951\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75847 - Weighted F1: 0.74444 - Macro F1: 0.43642\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74121 - Weighted F1: 0.72695 - Macro F1: 0.44793\n",
      "\n",
      "Training epoch 353\n",
      "===========================================================>    236    2.03829    0.77852    0.76641    0.45126\n",
      "Validating epoch 353\n",
      "===============>    058    2.06282    0.75409    0.74023    0.45510\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996410246235951\n",
      "         This LR: 0.0009964000664502409\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77525 - Weighted F1: 0.76319 - Macro F1: 0.44936\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74153 - Weighted F1: 0.72789 - Macro F1: 0.44751\n",
      "\n",
      "Training epoch 354\n",
      "===========================================================>    236    2.03210    0.78471    0.77262    0.45613\n",
      "Validating epoch 354\n",
      "===============>    058    2.05891    0.75800    0.74216    0.45752\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009964000664502409\n",
      "         This LR: 0.000996389886768532\n",
      "             TRAIN ----- Loss: 2.02 - Accuracy: 0.78141 - Weighted F1: 0.76937 - Macro F1: 0.45421\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.74537 - Weighted F1: 0.72979 - Macro F1: 0.44990\n",
      "\n",
      "Training epoch 355\n",
      "===========================================================>    236    2.03570    0.78111    0.76538    0.45267\n",
      "Validating epoch 355\n",
      "===============>    058    2.07049    0.74641    0.72230    0.44403\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996389886768532\n",
      "         This LR: 0.0009963797071908236\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77783 - Weighted F1: 0.76216 - Macro F1: 0.45076\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73397 - Weighted F1: 0.71026 - Macro F1: 0.43663\n",
      "\n",
      "Training epoch 356\n",
      "===========================================================>    236    2.03976    0.77704    0.75818    0.45136\n",
      "Validating epoch 356\n",
      "===============>    058    2.06292    0.75399    0.72849    0.45012\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963797071908236\n",
      "         This LR: 0.0009963695277171144\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77377 - Weighted F1: 0.75500 - Macro F1: 0.44946\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74143 - Weighted F1: 0.71635 - Macro F1: 0.44262\n",
      "\n",
      "Training epoch 357\n",
      "===========================================================>    236    2.03635    0.78046    0.76141    0.45611\n",
      "Validating epoch 357\n",
      "===============>    058    2.05473    0.76218    0.74309    0.45939\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963695277171144\n",
      "         This LR: 0.0009963593483474035\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77718 - Weighted F1: 0.75821 - Macro F1: 0.45420\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.74948 - Weighted F1: 0.73071 - Macro F1: 0.45174\n",
      "\n",
      "Training epoch 358\n",
      "===========================================================>    236    2.02962    0.78719    0.76926    0.45982\n",
      "Validating epoch 358\n",
      "===============>    058    2.05352    0.76339    0.74757    0.46116\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963593483474035\n",
      "         This LR: 0.0009963491690816895\n",
      "             TRAIN ----- Loss: 2.02 - Accuracy: 0.78388 - Weighted F1: 0.76603 - Macro F1: 0.45788\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.75067 - Weighted F1: 0.73512 - Macro F1: 0.45348\n",
      "\n",
      "Training epoch 359\n",
      "===========================================================>    236    2.02741    0.78939    0.77308    0.46065\n",
      "Validating epoch 359\n",
      "===============>    058    2.05345    0.76346    0.74618    0.46050\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963491690816895\n",
      "         This LR: 0.0009963389899199716\n",
      "             TRAIN ----- Loss: 2.02 - Accuracy: 0.78607 - Weighted F1: 0.76983 - Macro F1: 0.45871\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.75074 - Weighted F1: 0.73375 - Macro F1: 0.45283\n",
      "\n",
      "Training epoch 360\n",
      "===========================================================>    236    2.02798    0.78883    0.77214    0.46037\n",
      "Validating epoch 360\n",
      "===============>    058    2.05384    0.76307    0.74521    0.46021\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963389899199716\n",
      "         This LR: 0.0009963288108622488\n",
      "             TRAIN ----- Loss: 2.02 - Accuracy: 0.78552 - Weighted F1: 0.76890 - Macro F1: 0.45843\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.75035 - Weighted F1: 0.73279 - Macro F1: 0.45254\n",
      "\n",
      "Training epoch 361\n",
      "===========================================================>    236    2.03802    0.77878    0.76172    0.45333\n",
      "Validating epoch 361\n",
      "===============>    058    2.05513    0.76178    0.74436    0.45999\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963288108622488\n",
      "         This LR: 0.0009963186319085198\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77551 - Weighted F1: 0.75852 - Macro F1: 0.45143\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.74908 - Weighted F1: 0.73195 - Macro F1: 0.45232\n",
      "\n",
      "Training epoch 362\n",
      "===========================================================>    236    2.04573    0.77108    0.75163    0.44696\n",
      "Validating epoch 362\n",
      "===============>    058    2.05607    0.76084    0.74077    0.45784\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963186319085198\n",
      "         This LR: 0.0009963084530587839\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76784 - Weighted F1: 0.74847 - Macro F1: 0.44508\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.74816 - Weighted F1: 0.72843 - Macro F1: 0.45021\n",
      "\n",
      "Training epoch 363\n",
      "===========================================================>    236    2.04509    0.77172    0.75104    0.44780\n",
      "Validating epoch 363\n",
      "===============>    058    2.05595    0.76096    0.74142    0.45840\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009963084530587839\n",
      "         This LR: 0.0009962982743130396\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76847 - Weighted F1: 0.74788 - Macro F1: 0.44592\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.74828 - Weighted F1: 0.72906 - Macro F1: 0.45076\n",
      "\n",
      "Training epoch 364\n",
      "===========================================================>    236    2.04078    0.77603    0.75466    0.45039\n",
      "Validating epoch 364\n",
      "===============>    058    2.05987    0.75704    0.72496    0.46398\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962982743130396\n",
      "         This LR: 0.0009962880956712861\n",
      "             TRAIN ----- Loss: 2.03 - Accuracy: 0.77277 - Weighted F1: 0.75149 - Macro F1: 0.44849\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74442 - Weighted F1: 0.71287 - Macro F1: 0.45625\n",
      "\n",
      "Training epoch 365\n",
      "===========================================================>    236    2.05696    0.75985    0.73121    0.43870\n",
      "Validating epoch 365\n",
      "===============>    058    2.06304    0.75386    0.73539    0.45484\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962880956712861\n",
      "         This LR: 0.0009962779171335224\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75665 - Weighted F1: 0.72814 - Macro F1: 0.43686\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74130 - Weighted F1: 0.72313 - Macro F1: 0.44726\n",
      "\n",
      "Training epoch 366\n",
      "===========================================================>    236    2.04546    0.77134    0.74749    0.44544\n",
      "Validating epoch 366\n",
      "===============>    058    2.05754    0.75936    0.73853    0.45709\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962779171335224\n",
      "         This LR: 0.0009962677386997473\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76810 - Weighted F1: 0.74435 - Macro F1: 0.44357\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.74671 - Weighted F1: 0.72623 - Macro F1: 0.44947\n",
      "\n",
      "Training epoch 367\n",
      "===========================================================>    236    2.04416    0.77265    0.74769    0.44626\n",
      "Validating epoch 367\n",
      "===============>    058    2.05524    0.76167    0.74191    0.45900\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962677386997473\n",
      "         This LR: 0.0009962575603699596\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76940 - Weighted F1: 0.74455 - Macro F1: 0.44439\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.74898 - Weighted F1: 0.72954 - Macro F1: 0.45135\n",
      "\n",
      "Training epoch 368\n",
      "===========================================================>    236    2.04638    0.77043    0.74421    0.44552\n",
      "Validating epoch 368\n",
      "===============>    058    2.05339    0.76352    0.74045    0.45912\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962575603699596\n",
      "         This LR: 0.0009962473821441586\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76719 - Weighted F1: 0.74109 - Macro F1: 0.44365\n",
      "             EVAL  ----- Loss: 2.02 - Accuracy: 0.75079 - Weighted F1: 0.72811 - Macro F1: 0.45147\n",
      "\n",
      "Training epoch 369\n",
      "===========================================================>    236    2.04765    0.76916    0.74173    0.44451\n",
      "Validating epoch 369\n",
      "===============>    058    2.06009    0.75683    0.73775    0.45600\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962473821441586\n",
      "         This LR: 0.000996237204022343\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76592 - Weighted F1: 0.73861 - Macro F1: 0.44265\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74421 - Weighted F1: 0.72545 - Macro F1: 0.44840\n",
      "\n",
      "Training epoch 370\n",
      "===========================================================>    236    2.04698    0.76983    0.74738    0.44358\n",
      "Validating epoch 370\n",
      "===============>    058    2.06650    0.75041    0.72589    0.44694\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996237204022343\n",
      "         This LR: 0.0009962270260045116\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76659 - Weighted F1: 0.74424 - Macro F1: 0.44172\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73791 - Weighted F1: 0.71379 - Macro F1: 0.43949\n",
      "\n",
      "Training epoch 371\n",
      "===========================================================>    236    2.07029    0.74652    0.72085    0.42671\n",
      "Validating epoch 371\n",
      "===============>    058    2.08375    0.73316    0.70671    0.43400\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962270260045116\n",
      "         This LR: 0.0009962168480906637\n",
      "             TRAIN ----- Loss: 2.06 - Accuracy: 0.74338 - Weighted F1: 0.71782 - Macro F1: 0.42492\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72094 - Weighted F1: 0.69493 - Macro F1: 0.42676\n",
      "\n",
      "Training epoch 372\n",
      "===========================================================>    236    2.04951    0.76730    0.74610    0.44009\n",
      "Validating epoch 372\n",
      "===============>    058    2.08003    0.73688    0.70908    0.43510\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009962168480906637\n",
      "         This LR: 0.000996206670280798\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76408 - Weighted F1: 0.74297 - Macro F1: 0.43824\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.72459 - Weighted F1: 0.69726 - Macro F1: 0.42785\n",
      "\n",
      "Training epoch 373\n",
      "===========================================================>    236    2.05435    0.76246    0.73559    0.43545\n",
      "Validating epoch 373\n",
      "===============>    058    2.07623    0.74068    0.71179    0.43674\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996206670280798\n",
      "         This LR: 0.0009961964925749136\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75925 - Weighted F1: 0.73250 - Macro F1: 0.43362\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72834 - Weighted F1: 0.69993 - Macro F1: 0.42946\n",
      "\n",
      "Training epoch 374\n",
      "===========================================================>    236    2.05057    0.76623    0.74292    0.43797\n",
      "Validating epoch 374\n",
      "===============>    058    2.06288    0.75403    0.73082    0.45002\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009961964925749136\n",
      "         This LR: 0.000996186314973009\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76301 - Weighted F1: 0.73980 - Macro F1: 0.43613\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74146 - Weighted F1: 0.71864 - Macro F1: 0.44252\n",
      "\n",
      "Training epoch 375\n",
      "===========================================================>    236    2.05085    0.76596    0.73921    0.43751\n",
      "Validating epoch 375\n",
      "===============>    058    2.07186    0.74506    0.72194    0.44328\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996186314973009\n",
      "         This LR: 0.000996176137475084\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76274 - Weighted F1: 0.73611 - Macro F1: 0.43567\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73264 - Weighted F1: 0.70990 - Macro F1: 0.43589\n",
      "\n",
      "Training epoch 376\n",
      "===========================================================>    236    2.05583    0.76097    0.73637    0.43435\n",
      "Validating epoch 376\n",
      "===============>    058    2.06863    0.74828    0.72707    0.44501\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996176137475084\n",
      "         This LR: 0.0009961659600811366\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75778 - Weighted F1: 0.73327 - Macro F1: 0.43252\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73581 - Weighted F1: 0.71495 - Macro F1: 0.43760\n",
      "\n",
      "Training epoch 377\n",
      "===========================================================>    236    2.06346    0.75334    0.73378    0.42980\n",
      "Validating epoch 377\n",
      "===============>    058    2.06349    0.75342    0.73364    0.45044\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009961659600811366\n",
      "         This LR: 0.0009961557827911662\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75017 - Weighted F1: 0.73069 - Macro F1: 0.42799\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74086 - Weighted F1: 0.72141 - Macro F1: 0.44293\n",
      "\n",
      "Training epoch 378\n",
      "===========================================================>    236    2.06348    0.75332    0.73510    0.43263\n",
      "Validating epoch 378\n",
      "===============>    058    2.07040    0.74651    0.72979    0.44887\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009961557827911662\n",
      "         This LR: 0.0009961456056051718\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75016 - Weighted F1: 0.73201 - Macro F1: 0.43081\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.73407 - Weighted F1: 0.71763 - Macro F1: 0.44139\n",
      "\n",
      "Training epoch 379\n",
      "===========================================================>    236    2.07654    0.74026    0.72434    0.42566\n",
      "Validating epoch 379\n",
      "===============>    058    2.08786    0.72904    0.70705    0.43058\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009961456056051718\n",
      "         This LR: 0.000996135428523152\n",
      "             TRAIN ----- Loss: 2.07 - Accuracy: 0.73715 - Weighted F1: 0.72130 - Macro F1: 0.42387\n",
      "             EVAL  ----- Loss: 2.05 - Accuracy: 0.71689 - Weighted F1: 0.69526 - Macro F1: 0.42341\n",
      "\n",
      "Training epoch 380\n",
      "===========================================================>    236    2.05640    0.76041    0.74287    0.43606\n",
      "Validating epoch 380\n",
      "===============>    058    2.06527    0.75164    0.73210    0.45089\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996135428523152\n",
      "         This LR: 0.0009961252515451062\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75721 - Weighted F1: 0.73975 - Macro F1: 0.43423\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73911 - Weighted F1: 0.71990 - Macro F1: 0.44337\n",
      "\n",
      "Training epoch 381\n",
      "===========================================================>    236    2.05855    0.75826    0.73872    0.43398\n",
      "Validating epoch 381\n",
      "===============>    058    2.06291    0.75400    0.73642    0.45401\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009961252515451062\n",
      "         This LR: 0.000996115074671033\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75507 - Weighted F1: 0.73562 - Macro F1: 0.43216\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74143 - Weighted F1: 0.72415 - Macro F1: 0.44645\n",
      "\n",
      "Training epoch 382\n",
      "===========================================================>    236    2.05302    0.76378    0.74025    0.43539\n",
      "Validating epoch 382\n",
      "===============>    058    2.06434    0.75257    0.73518    0.45325\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996115074671033\n",
      "         This LR: 0.0009961048979009314\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76057 - Weighted F1: 0.73714 - Macro F1: 0.43356\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74003 - Weighted F1: 0.72293 - Macro F1: 0.44570\n",
      "\n",
      "Training epoch 383\n",
      "===========================================================>    236    2.05680    0.76000    0.73676    0.43325\n",
      "Validating epoch 383\n",
      "===============>    058    2.06475    0.75216    0.73558    0.45342\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009961048979009314\n",
      "         This LR: 0.0009960947212348003\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75681 - Weighted F1: 0.73367 - Macro F1: 0.43143\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73962 - Weighted F1: 0.72332 - Macro F1: 0.44587\n",
      "\n",
      "Training epoch 384\n",
      "===========================================================>    236    2.05438    0.76243    0.74199    0.43540\n",
      "Validating epoch 384\n",
      "===============>    058    2.06598    0.75093    0.73406    0.45230\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960947212348003\n",
      "         This LR: 0.0009960845446726388\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75923 - Weighted F1: 0.73887 - Macro F1: 0.43357\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73842 - Weighted F1: 0.72182 - Macro F1: 0.44476\n",
      "\n",
      "Training epoch 385\n",
      "===========================================================>    236    2.05472    0.76208    0.73577    0.43330\n",
      "Validating epoch 385\n",
      "===============>    058    2.06358    0.75333    0.73637    0.45330\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960845446726388\n",
      "         This LR: 0.0009960743682144457\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75888 - Weighted F1: 0.73268 - Macro F1: 0.43148\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74078 - Weighted F1: 0.72409 - Macro F1: 0.44575\n",
      "\n",
      "Training epoch 386\n",
      "===========================================================>    236    2.05323    0.76357    0.74062    0.43492\n",
      "Validating epoch 386\n",
      "===============>    058    2.06353    0.75338    0.73402    0.45115\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960743682144457\n",
      "         This LR: 0.00099606419186022\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76036 - Weighted F1: 0.73750 - Macro F1: 0.43309\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74082 - Weighted F1: 0.72178 - Macro F1: 0.44363\n",
      "\n",
      "Training epoch 387\n",
      "===========================================================>    236    2.05936    0.75745    0.73497    0.43101\n",
      "Validating epoch 387\n",
      "===============>    058    2.06641    0.75050    0.73320    0.45064\n",
      "     Epoch stats:\n",
      "         Last LR: 0.00099606419186022\n",
      "         This LR: 0.0009960540156099607\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75427 - Weighted F1: 0.73189 - Macro F1: 0.42920\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73799 - Weighted F1: 0.72098 - Macro F1: 0.44313\n",
      "\n",
      "Training epoch 388\n",
      "===========================================================>    236    2.05782    0.75899    0.73790    0.43257\n",
      "Validating epoch 388\n",
      "===============>    058    2.06671    0.75021    0.73303    0.44957\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960540156099607\n",
      "         This LR: 0.0009960438394636666\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75580 - Weighted F1: 0.73480 - Macro F1: 0.43075\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73770 - Weighted F1: 0.72081 - Macro F1: 0.44208\n",
      "\n",
      "Training epoch 389\n",
      "===========================================================>    236    2.05726    0.75954    0.73834    0.43338\n",
      "Validating epoch 389\n",
      "===============>    058    2.06812    0.74879    0.73389    0.45191\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960438394636666\n",
      "         This LR: 0.0009960336634213366\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75635 - Weighted F1: 0.73524 - Macro F1: 0.43156\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.73631 - Weighted F1: 0.72166 - Macro F1: 0.44438\n",
      "\n",
      "Training epoch 390\n",
      "===========================================================>    236    2.05744    0.75937    0.73931    0.43398\n",
      "Validating epoch 390\n",
      "===============>    058    2.06257    0.75434    0.73745    0.45422\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960336634213366\n",
      "         This LR: 0.0009960234874829698\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75618 - Weighted F1: 0.73620 - Macro F1: 0.43216\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74177 - Weighted F1: 0.72516 - Macro F1: 0.44665\n",
      "\n",
      "Training epoch 391\n",
      "===========================================================>    236    2.05728    0.75952    0.73496    0.43631\n",
      "Validating epoch 391\n",
      "===============>    058    2.06196    0.75495    0.72658    0.45343\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960234874829698\n",
      "         This LR: 0.000996013311648565\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75633 - Weighted F1: 0.73187 - Macro F1: 0.43447\n",
      "             EVAL  ----- Loss: 2.03 - Accuracy: 0.74237 - Weighted F1: 0.71447 - Macro F1: 0.44588\n",
      "\n",
      "Training epoch 392\n",
      "===========================================================>    236    2.05929    0.75752    0.72922    0.43395\n",
      "Validating epoch 392\n",
      "===============>    058    2.07596    0.74095    0.71135    0.43668\n",
      "     Epoch stats:\n",
      "         Last LR: 0.000996013311648565\n",
      "         This LR: 0.0009960031359181212\n",
      "             TRAIN ----- Loss: 2.05 - Accuracy: 0.75434 - Weighted F1: 0.72616 - Macro F1: 0.43212\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72860 - Weighted F1: 0.69950 - Macro F1: 0.42940\n",
      "\n",
      "Training epoch 393\n",
      "===========================================================>    236    2.05256    0.76424    0.73910    0.43988\n",
      "Validating epoch 393\n",
      "===============>    058    1.96798    0.84892    0.83502    0.53182\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009960031359181212\n",
      "         This LR: 0.0009959929602916375\n",
      "             TRAIN ----- Loss: 2.04 - Accuracy: 0.76103 - Weighted F1: 0.73600 - Macro F1: 0.43803\n",
      "             EVAL  ----- Loss: 1.94 - Accuracy: 0.83478 - Weighted F1: 0.82111 - Macro F1: 0.52296\n",
      "\n",
      "Training epoch 394\n",
      "===========================================================>    236    2.03273    0.78408    0.76704    0.45350\n",
      "Validating epoch 394\n",
      "===============>    058    2.07726    0.73965    0.71524    0.44062\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009959929602916375\n",
      "         This LR: 0.0009959827847691126\n",
      "             TRAIN ----- Loss: 2.02 - Accuracy: 0.78079 - Weighted F1: 0.76382 - Macro F1: 0.45159\n",
      "             EVAL  ----- Loss: 2.04 - Accuracy: 0.72732 - Weighted F1: 0.70332 - Macro F1: 0.43328\n",
      "\n",
      "Training epoch 395\n",
      "===========================================================>    236    2.01998    0.79682    0.78390    0.46354\n",
      "Validating epoch 395\n",
      "===============>    058    1.96329    0.85362    0.84132    0.53543\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009959827847691126\n",
      "         This LR: 0.0009959726093505456\n",
      "             TRAIN ----- Loss: 2.01 - Accuracy: 0.79348 - Weighted F1: 0.78061 - Macro F1: 0.46159\n",
      "             EVAL  ----- Loss: 1.93 - Accuracy: 0.83940 - Weighted F1: 0.82730 - Macro F1: 0.52651\n",
      "\n",
      "Training epoch 396\n",
      "===========================================================>    236    2.00576    0.81104    0.79545    0.46808\n",
      "Validating epoch 396\n",
      "===============>    058    2.00507    0.81185    0.79936    0.49541\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009959726093505456\n",
      "         This LR: 0.0009959624340359354\n",
      "             TRAIN ----- Loss: 2.00 - Accuracy: 0.80763 - Weighted F1: 0.79211 - Macro F1: 0.46611\n",
      "             EVAL  ----- Loss: 1.97 - Accuracy: 0.79832 - Weighted F1: 0.78604 - Macro F1: 0.48715\n",
      "\n",
      "Training epoch 397\n",
      "===========================================================>    236    2.00813    0.80867    0.79327    0.46849\n",
      "Validating epoch 397\n",
      "===============>    058    2.03556    0.78135    0.76533    0.46640\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009959624340359354\n",
      "         This LR: 0.0009959522588252807\n",
      "             TRAIN ----- Loss: 2.00 - Accuracy: 0.80528 - Weighted F1: 0.78994 - Macro F1: 0.46652\n",
      "             EVAL  ----- Loss: 2.00 - Accuracy: 0.76833 - Weighted F1: 0.75258 - Macro F1: 0.45862\n",
      "\n",
      "Training epoch 398\n",
      "===========================================================>    236    2.01168    0.80513    0.78786    0.46537\n",
      "Validating epoch 398\n",
      "===============>    058    2.02687    0.79002    0.78040    0.48462\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009959522588252807\n",
      "         This LR: 0.0009959420837185808\n",
      "             TRAIN ----- Loss: 2.00 - Accuracy: 0.80175 - Weighted F1: 0.78455 - Macro F1: 0.46341\n",
      "             EVAL  ----- Loss: 1.99 - Accuracy: 0.77685 - Weighted F1: 0.76739 - Macro F1: 0.47654\n",
      "\n",
      "Training epoch 399\n",
      "===========================================================>    236    2.00876    0.80805    0.78995    0.46603\n",
      "Validating epoch 399\n",
      "===============>    058    1.99703    0.81989    0.80809    0.51111\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009959420837185808\n",
      "         This LR: 0.0009959319087158344\n",
      "             TRAIN ----- Loss: 2.00 - Accuracy: 0.80465 - Weighted F1: 0.78663 - Macro F1: 0.46407\n",
      "             EVAL  ----- Loss: 1.96 - Accuracy: 0.80622 - Weighted F1: 0.79462 - Macro F1: 0.50259\n",
      "\n",
      "Training epoch 400\n",
      "===========================================================>    236    2.01919    0.79762    0.78599    0.46572\n",
      "Validating epoch 400\n",
      "===============>    058    2.04679    0.77012    0.75357    0.46988\n",
      "     Epoch stats:\n",
      "         Last LR: 0.0009959319087158344\n",
      "         This LR: 0.0009959217338170404\n",
      "             TRAIN ----- Loss: 2.01 - Accuracy: 0.79427 - Weighted F1: 0.78269 - Macro F1: 0.46377\n",
      "             EVAL  ----- Loss: 2.01 - Accuracy: 0.75728 - Weighted F1: 0.74102 - Macro F1: 0.46204\n",
      "\n",
      "Training epoch 401\n",
      "==============--------------------------------------------->    055    2.03254    0.78396    0.77410    0.46333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14184/3892224949.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartingEpoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTraining epoch \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_weighted_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_macro_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_total\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nValidating epoch \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0meval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_weighted_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_macro_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_total\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluating_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14184/1822482495.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(gen, epoch_index)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\TFG\\GNN-NIDS-main\\GNN_NIDS_pytorch\\IDS2017_Dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m                                 \u001b[1;31m#try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                                     \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraces_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_time_traces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                                     \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                                     \u001b[1;31m# We do not need to do the undersampling here, since it was done during the preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\TFG\\GNN-NIDS-main\\GNN_NIDS_pytorch\\generator.py\u001b[0m in \u001b[0;36mgraph_to_dict\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mconnection_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconnection_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;31m# obtain the labels of the nodes (indicator r.v indicating whether it has been infected or not)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(GNN)\n",
    "first = True\n",
    "lastLR = 0.0\n",
    "for epoch in range(startingEpoch, epochs+1):\n",
    "    print(\"\\nTraining epoch \" + str(epoch))\n",
    "    (train_acc, train_loss, train_weighted_f1, train_macro_f1, train_total) = train_epoch(training_generator, epoch)\n",
    "    print(\"\\nValidating epoch \" + str(epoch))\n",
    "    (eval_acc, eval_loss, eval_weighted_f1, eval_macro_f1, eval_total) = validate_epoch(evaluating_generator, epoch)\n",
    "    thisLR = lr * decay_rate ** (epoch / decay_steps)\n",
    "    # scheduler.step()\n",
    "    print(\"\")\n",
    "    print(\"     Epoch stats:\")\n",
    "    print(\"         Last LR: \" + str(lastLR))\n",
    "    print(\"         This LR: \" + str(thisLR))\n",
    "    lastLR = thisLR\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = thisLR\n",
    "    print(\"             TRAIN ----- Loss: \" + \"{:.2f}\".format(train_loss) + \" - Accuracy: \" + \"{:.5f}\".format(train_acc)\n",
    "        + \" - Weighted F1: \" + \"{:.5f}\".format(train_weighted_f1) + \" - Macro F1: \" + \"{:.5f}\".format(train_macro_f1))\n",
    "    print(\"             EVAL  ----- Loss: \" + \"{:.2f}\".format(eval_loss) + \" - Accuracy: \" + \"{:.5f}\".format(eval_acc)\n",
    "        + \" - Weighted F1: \" + \"{:.5f}\".format(eval_weighted_f1) + \" - Macro F1: \" + \"{:.5f}\".format(eval_macro_f1))\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': eval_loss,\n",
    "            'accuracy': eval_acc\n",
    "            }, checkpoint_path + \"/weights.\" + \"{0}-\".format(str(epoch).zfill(4)) + \"{:.2f}\".format(eval_loss) + \".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
